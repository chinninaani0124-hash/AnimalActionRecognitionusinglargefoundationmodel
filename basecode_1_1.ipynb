{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNjtHCoTevfO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/dataset\""
      ],
      "metadata": {
        "id": "mmc-2t8xfAq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/videos\n",
        "!tar -xvzf \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/dataset/video.tar.gz\" -C /content/videos"
      ],
      "metadata": {
        "id": "DSSyw6tzfHzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "video_dir = \"/content/videos/video\"\n",
        "video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
        "print(f\"Found {len(video_files)} video files.\")\n",
        "print(video_files[:5])  # show a few"
      ],
      "metadata": {
        "id": "Vc5a3WNQfNkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "df = pd.read_excel(metadata_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "Oo98mqC2fOcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformers for BLIP\n",
        "!pip install transformers\n",
        "\n",
        "#Torch for model and inference\n",
        "!pip install torch torchvision torchaudio\n",
        "\n",
        "#Excel file reading\n",
        "!pip install openpyxl pandas\n",
        "\n",
        "#Image and video processing\n",
        "!pip install opencv-python pillow"
      ],
      "metadata": {
        "id": "33-kW6mPfWJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IKiw1VKVumbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline execution"
      ],
      "metadata": {
        "id": "EgI5vmASunJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "def caption_image(image_path):\n",
        "    raw_image = Image.open(image_path).convert('RGB')\n",
        "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
        "    output = blip_model.generate(**inputs)\n",
        "    return processor.decode(output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "RBWO6yZUTMXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def extract_frames_1fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)  # Get video FPS\n",
        "    interval = int(fps)  # Capture 1 frame per second\n",
        "\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    print(f\"Extracted {saved} frames (1 fps) from {video_path}\")\n"
      ],
      "metadata": {
        "id": "a8mmudbwTqFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_video_captions(frames_folder):\n",
        "    captions = []\n",
        "    for frame in sorted(os.listdir(frames_folder)):\n",
        "        if frame.endswith(\".jpg\"):\n",
        "            caption = caption_image(os.path.join(frames_folder, frame))\n",
        "            captions.append(caption)\n",
        "    return captions\n"
      ],
      "metadata": {
        "id": "_gYs7WSGTxty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n"
      ],
      "metadata": {
        "id": "LGKjebw_Xq6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_captions_bart_single(captions):\n",
        "    # Provide a clear instruction\n",
        "    text_input = \" \".join(captions)\n",
        "    text_input = f\"Generate one concise descriptive sentence: {text_input}\"\n",
        "\n",
        "    inputs = tokenizer_bart([text_input], max_length=1024, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    summary_ids = model_bart.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=25,   # restrict length\n",
        "        min_length=5,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    result = tokenizer_bart.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Ensure single sentence\n",
        "    return result.split(\".\")[0].strip() + \".\""
      ],
      "metadata": {
        "id": "7Kk4c11JcGcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/videos/video/LKBDONQN.mp4\"\n",
        "frames_folder = \"/content/frames\"\n",
        "\n",
        "# Step 1: Extract frames\n",
        "extract_frames_1fps(video_path, frames_folder)\n",
        "\n",
        "# Step 2: Generate captions\n",
        "frame_captions = generate_video_captions(frames_folder)\n",
        "print(\"Frame Captions:\", frame_captions)\n",
        "\n",
        "# Step 3: Summarize with BART (single sentence)\n",
        "final_caption = summarize_captions_bart_single(frame_captions)\n",
        "print(\"Final Video Caption:\", final_caption)\n"
      ],
      "metadata": {
        "id": "8Hk3J1cTUvvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load metadata Excel\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "df = pd.read_excel(metadata_path)\n",
        "metadata_dict = {row['video_id']: row for _, row in df.iterrows()}\n"
      ],
      "metadata": {
        "id": "LAwsVoG8jV2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example final caption from your BLIP + BART pipeline\n",
        "final_caption = \"Generate one concise descriptive sentence: a small bird perched on a branch of a tree\"\n",
        "video_id = \"LKBDONQN\"\n",
        "\n",
        "# Fetch metadata for this video\n",
        "meta = metadata_dict.get(video_id, {})\n",
        "\n",
        "# Extract animals and actions keywords\n",
        "animals = meta.get(\"list_animal\", [])\n",
        "\n",
        "# Parse actions if stored as a string representation of list of tuples\n",
        "actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "try:\n",
        "    actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "except:\n",
        "    actions = []\n",
        "\n",
        "keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "\n",
        "# Check semantic correctness\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if len(keywords) > 0 else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "percentage = score * 100\n",
        "\n",
        "print(f\"Video ID: {video_id}\")\n",
        "print(f\"Final Caption: {final_caption}\")\n",
        "print(f\"Keywords: {keywords}\")\n",
        "print(f\"Matched Keywords: {matched}\")\n",
        "print(f\"Missing Keywords: {missing}\")\n",
        "print(f\"Semantic Correctness Score: {score:.2f} ({percentage:.1f}%)\")\n"
      ],
      "metadata": {
        "id": "lrTNsiS3jHBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bKS0bWq_kCF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers timm fairscale accelerate opencv-python pandas\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, BartTokenizer, BartForConditionalGeneration\n",
        "import pandas as pd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# BLIP Setup\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "def caption_image(image_path):\n",
        "    raw_image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(raw_image, return_tensors=\"pt\").to(device)\n",
        "    out = blip_model.generate(**inputs)\n",
        "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# BART Setup\n",
        "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def summarize_captions_bart(captions):\n",
        "    text_input = \" \".join(captions)\n",
        "    text_input = f\"Generate one concise descriptive sentence: {text_input}\"\n",
        "    inputs = tokenizer_bart([text_input], max_length=1024, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    summary_ids = model_bart.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=25,\n",
        "        min_length=5,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer_bart.decode(summary_ids[0], skip_special_tokens=True).split(\".\")[0].strip() + \".\"\n"
      ],
      "metadata": {
        "id": "vYnUIzAYkBwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames_1fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps) if fps > 0 else 1  # 1 frame per second\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n"
      ],
      "metadata": {
        "id": "CXKvJMrNkfVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_video_captions(frames_folder):\n",
        "    captions = []\n",
        "    for frame in sorted(os.listdir(frames_folder)):\n",
        "        if frame.endswith(\".jpg\"):\n",
        "            caption = caption_image(os.path.join(frames_folder, frame))\n",
        "            captions.append(caption)\n",
        "    return captions\n"
      ],
      "metadata": {
        "id": "-o3qmaI7kmhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if len(keywords) > 0 else 0\n",
        "    return score, matched, missing\n"
      ],
      "metadata": {
        "id": "F9fBv2wfkqot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IIuoq-jmlC9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "18NV-61MmGVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8Edlf3r3C2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLIP+BART"
      ],
      "metadata": {
        "id": "qKZB1J0U3DaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Install dependencies\n",
        "# ================================\n",
        "!pip install transformers timm fairscale accelerate opencv-python pandas\n",
        "\n",
        "# ================================\n",
        "# 2. Import Libraries\n",
        "# ================================\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# ================================\n",
        "# 3. Device Setup\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4. BLIP Setup\n",
        "# ================================\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "def caption_image(image_path):\n",
        "    raw_image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(raw_image, return_tensors=\"pt\").to(device)\n",
        "    out = blip_model.generate(**inputs)\n",
        "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# ================================\n",
        "# 5. BART Setup\n",
        "# ================================\n",
        "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def summarize_captions_bart(captions):\n",
        "    text_input = \" \".join(captions)\n",
        "    text_input = f\"Generate one concise descriptive sentence: {text_input}\"\n",
        "    inputs = tokenizer_bart([text_input], max_length=1024, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    summary_ids = model_bart.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=25,\n",
        "        min_length=5,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer_bart.decode(summary_ids[0], skip_special_tokens=True).split(\".\")[0].strip() + \".\"\n",
        "\n",
        "# ================================\n",
        "# 6. Frame Extraction (1 FPS)\n",
        "# ================================\n",
        "def extract_frames_1fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps) if fps > 0 else 1\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7. Generate Captions for Frames\n",
        "# ================================\n",
        "def generate_video_captions(frames_folder):\n",
        "    captions = []\n",
        "    for frame in sorted(os.listdir(frames_folder)):\n",
        "        if frame.endswith(\".jpg\"):\n",
        "            caption = caption_image(os.path.join(frames_folder, frame))\n",
        "            captions.append(caption)\n",
        "    return captions\n",
        "\n",
        "# ================================\n",
        "# 8. Semantic Correctness\n",
        "# ================================\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if len(keywords) > 0 else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "# ================================\n",
        "# 9. Load Metadata\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "df = pd.read_excel(metadata_path)\n",
        "metadata_dict = {row['video_id']: row for _, row in df.iterrows()}\n",
        "\n",
        "# ================================\n",
        "# 10. Process Batch of 20 Videos\n",
        "# ================================\n",
        "video_folder = \"/content/videos/video\"\n",
        "video_list = sorted(os.listdir(video_folder))[:20]\n",
        "\n",
        "results = []\n",
        "\n",
        "for video_name in video_list:\n",
        "    video_id = os.path.splitext(video_name)[0]\n",
        "    video_path = os.path.join(video_folder, video_name)\n",
        "    frames_folder = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\nProcessing video: {video_name}\")\n",
        "\n",
        "    # Step 1: Extract frames\n",
        "    extracted = extract_frames_1fps(video_path, frames_folder)\n",
        "    print(f\"Extracted {extracted} frames (1 fps) from {video_name}\")\n",
        "\n",
        "    # Step 2: Generate BLIP captions\n",
        "    frame_captions = generate_video_captions(frames_folder)\n",
        "    print(\"Frame Captions:\", frame_captions)\n",
        "\n",
        "    # Step 3: Summarize with BART\n",
        "    final_caption = summarize_captions_bart(frame_captions)\n",
        "    print(\"Final Video Caption:\", final_caption)\n",
        "\n",
        "    # Step 4: Semantic correctness\n",
        "    meta = metadata_dict.get(video_id, {})\n",
        "    animals = meta.get(\"list_animal\", [])\n",
        "    actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "    try:\n",
        "        actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "    except:\n",
        "        actions = []\n",
        "\n",
        "    keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions\": frame_captions,\n",
        "        \"keywords\": keywords,\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 11. Save Results\n",
        "# ================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/batch20_results.csv\", index=False)\n",
        "print(\"\\nBatch processing complete! Results saved to /content/batch20_results.csv\")\n"
      ],
      "metadata": {
        "id": "9zyOinW-mGvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load previously generated batch results\n",
        "batch_results_path = \"/content/batch20_results.csv\"\n",
        "batch_df = pd.read_csv(batch_results_path)\n",
        "\n",
        "# Load metadata\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "metadata_df = pd.read_excel(metadata_path)\n",
        "metadata_dict = {row['video_id']: row for _, row in metadata_df.iterrows()}\n",
        "\n",
        "# Semantic correctness function (keyword-based)\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if len(keywords) > 0 else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "# Process each video\n",
        "semantic_results = []\n",
        "\n",
        "for idx, row in batch_df.iterrows():\n",
        "    video_id = row[\"video_id\"]\n",
        "    final_caption = row[\"final_caption\"]\n",
        "\n",
        "    # Get metadata for this video\n",
        "    meta = metadata_dict.get(video_id, {})\n",
        "    animals = meta.get(\"list_animal\", [])\n",
        "    actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "    try:\n",
        "        actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "    except:\n",
        "        actions = []\n",
        "\n",
        "    keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "\n",
        "    # Compute correctness\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    semantic_results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"keywords\": keywords,\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame and save\n",
        "semantic_df = pd.DataFrame(semantic_results)\n",
        "semantic_df.to_csv(\"/content/semantic_correctness_batch20.csv\", index=False)\n",
        "\n",
        "print(\"Semantic correctness evaluation complete!\")\n",
        "print(semantic_df[[\"video_id\", \"semantic_correctness_percent\", \"matched_keywords\", \"missing_keywords\"]])\n"
      ],
      "metadata": {
        "id": "p9tGy6xsm8kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UNPI3DKd21B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhBNJK-3KRQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "alDCc_6AKRE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLIP + T5"
      ],
      "metadata": {
        "id": "kS0XBtmZgwC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core installations for BLIP, image processing, and LLM summarization\n",
        "!pip install git+https://github.com/salesforce/BLIP.git\n",
        "!pip install transformers\n",
        "!pip install timm\n",
        "!pip install opencv-python\n",
        "!pip install pillow\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "id": "BkQnNXeRgwdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def extract_frames_1fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps)  # 1 frame per second\n",
        "\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "\n",
        "    vidcap.release()\n",
        "    print(f\" Extracted {saved} frames (1 fps) from: {video_path}\")\n",
        "\n",
        "# Video path\n",
        "video_path = \"/content/videos/video/LKBDONQN.mp4\"\n",
        "frames_folder = \"/content/frames/LKBDONQN\"\n",
        "extract_frames_1fps(video_path, frames_folder)\n"
      ],
      "metadata": {
        "id": "_rixdB1Xia1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
        "    out = model.generate(**inputs)\n",
        "    return processor.decode(out[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "ZT8fzQ8zkLu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "frames_folder = \"/content/frames/LKBDONQN\"\n",
        "captions = []\n",
        "\n",
        "# List frames in sorted order\n",
        "frame_files = sorted(f for f in os.listdir(frames_folder) if f.endswith(\".jpg\"))\n",
        "\n",
        "for fname in frame_files:\n",
        "    path = os.path.join(frames_folder, fname)\n",
        "    caption = generate_caption(path)\n",
        "    captions.append((fname, caption))\n",
        "    print(f\"{fname}: {caption}\")\n"
      ],
      "metadata": {
        "id": "5pBu8pj5l46X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load FLAN-T5\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(device)\n",
        "\n",
        "def summarize_captions(captions):\n",
        "    # Extract just the caption strings\n",
        "    caption_texts = [cap for _, cap in captions]\n",
        "    combined_text = \" \".join(caption_texts)\n",
        "\n",
        "    # Enhanced prompt for better detail retention\n",
        "    prompt = (\n",
        "    \"From the following frame captions, write one caption that combines all the information. \"\n",
        "    \"Do not lose any specific detail such as color, position, or object type. Be precise and explicit: \"\n",
        "    f\"{combined_text}\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # Tokenize and summarize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    outputs = model.generate(**inputs, max_length=60, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Example usage (assuming 'captions' is already defined)\n",
        "final_caption = summarize_captions(captions)\n",
        "print(\"\\n Final Video-Level Caption:\\n\", final_caption)\n"
      ],
      "metadata": {
        "id": "OV6rpmCXl9j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# === Load BLIP-2 model (OPT 2.7B) ===\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# === Load FLAN-T5 ===\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(device)\n",
        "\n",
        "# === Frame Extraction ===\n",
        "def extract_frames_1fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps)\n",
        "    success, image = vidcap.read()\n",
        "    count = saved = 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            fpath = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(fpath, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n",
        "\n",
        "# === Caption with BLIP-2 ===\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device, torch.float16)\n",
        "    outputs = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    caption = blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    return caption\n",
        "\n",
        "# === Summarize with FLAN-T5 ===\n",
        "def summarize_captions(captions_list):\n",
        "    combined = \" \".join(captions_list)\n",
        "    prompt = (\n",
        "        \"Generate a single, highly descriptive and detailed caption from the following frame captions. \"\n",
        "        \"Retain unique information like color, size, and position: \" + combined\n",
        "    )\n",
        "    inputs = t5_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    outputs = t5_model.generate(**inputs, max_length=30, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# === MAIN PROCESSING ===\n",
        "video_dir = \"/content/videos/video/\"\n",
        "frames_root = \"/content/frames/\"\n",
        "frame_caption_data = []\n",
        "video_caption_data = []\n",
        "\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:20]\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_folder = os.path.join(frames_root, video_id)\n",
        "\n",
        "    print(f\"\\n Processing: {video_file}\")\n",
        "    n_frames = extract_frames_1fps(video_path, frame_folder)\n",
        "    print(f\"  → Extracted {n_frames} frames.\")\n",
        "\n",
        "    frame_captions = []\n",
        "    for fname in sorted(os.listdir(frame_folder)):\n",
        "        if fname.endswith(\".jpg\"):\n",
        "            fpath = os.path.join(frame_folder, fname)\n",
        "            caption = generate_caption(fpath)\n",
        "            frame_captions.append(caption)\n",
        "            frame_caption_data.append([video_id, fname, caption])\n",
        "            print(f\"    {fname}: {caption}\")\n",
        "\n",
        "    final_caption = summarize_captions(frame_captions)\n",
        "    video_caption_data.append([video_id, final_caption])\n",
        "    print(f\" Final Caption: {final_caption}\")\n",
        "\n",
        "# === SAVE OUTPUT ===\n",
        "frame_df = pd.DataFrame(frame_caption_data, columns=[\"video_id\", \"frame\", \"frame_caption\"])\n",
        "frame_df.to_csv(\"/content/frame_level_captions.csv\", index=False)\n",
        "\n",
        "video_df = pd.DataFrame(video_caption_data, columns=[\"video_id\", \"video_caption\"])\n",
        "video_df.to_csv(\"/content/video_level_captions.csv\", index=False)\n",
        "\n",
        "print(\"\\n Saved:\")\n",
        "print(\" - /content/frame_level_captions.csv\")\n",
        "print(\" - /content/video_level_captions.csv\")\n"
      ],
      "metadata": {
        "id": "G1UZn0pdrbKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"/content/frame_level_captions.csv\")\n",
        "files.download(\"/content/video_level_captions.csv\")\n"
      ],
      "metadata": {
        "id": "VkT4AN-ouJXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === Load Outputs ===\n",
        "video_df = pd.read_csv(\"/content/video_level_captions.csv\")\n",
        "\n",
        "# === Load Metadata ===\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "metadata_df = pd.read_excel(metadata_path)\n",
        "\n",
        "# Convert metadata to dictionary for quick lookup\n",
        "metadata_dict = {row['video_id']: row for _, row in metadata_df.iterrows()}\n",
        "\n",
        "# === Semantic Matching Function ===\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if len(keywords) > 0 else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "# === Evaluate Each Video ===\n",
        "semantic_results = []\n",
        "\n",
        "for idx, row in video_df.iterrows():\n",
        "    video_id = row[\"video_id\"]\n",
        "    final_caption = row[\"video_caption\"]\n",
        "\n",
        "    # Get metadata\n",
        "    meta = metadata_dict.get(video_id, {})\n",
        "    animals = meta.get(\"list_animal\", [])\n",
        "    actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "\n",
        "    # Parse list_animal_action (a stringified list of tuples)\n",
        "    try:\n",
        "        actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "    except:\n",
        "        actions = []\n",
        "\n",
        "    # Combine animal and action keywords\n",
        "    keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "\n",
        "    # Score the caption\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    semantic_results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"video_caption\": final_caption,\n",
        "        \"keywords\": keywords,\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# === Save and View Results ===\n",
        "semantic_df = pd.DataFrame(semantic_results)\n",
        "semantic_df.to_csv(\"/content/semantic_correctness_blip_flan.csv\", index=False)\n",
        "\n",
        "print(\"✅ Semantic correctness evaluation complete!\\n\")\n",
        "print(semantic_df[[\"video_id\", \"semantic_correctness_percent\", \"matched_keywords\", \"missing_keywords\"]])\n"
      ],
      "metadata": {
        "id": "GhnZ7jN9uMDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbWDOJFgzXwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fGKx5J4HzXtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fml1P5RxzXqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLIP2+BART"
      ],
      "metadata": {
        "id": "C_fl1t1p3S6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Install Dependencies\n",
        "# ================================\n",
        "# !pip install git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas -q\n",
        "\n",
        "# ================================\n",
        "# 2. Import Libraries\n",
        "# ================================\n",
        "import os, cv2, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 3. Device Setup\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4. BLIP2 Setup (Captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device, torch.float16)\n",
        "    outputs = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    return blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# ================================\n",
        "# 5. BART Setup (Summarization)\n",
        "# ================================\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def summarize_captions_bart(captions):\n",
        "    text_input = \" \".join(captions)\n",
        "    prompt = f\"Generate one concise descriptive sentence: {text_input}\"\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=25,\n",
        "        min_length=5,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).split(\".\")[0].strip() + \".\"\n",
        "\n",
        "# ================================\n",
        "# 6. Frame Extraction\n",
        "# ================================\n",
        "def extract_frames_1fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps) if fps > 0 else 1\n",
        "    success, image = cap.read()\n",
        "    count = saved = 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            fpath = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(fpath, image)\n",
        "            saved += 1\n",
        "        success, image = cap.read()\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7. Metadata Matching Function\n",
        "# ================================\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if keywords else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "# ================================\n",
        "# 8. Load Metadata\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "metadata_dict = {row[\"video_id\"]: row for _, row in meta_df.iterrows()}\n",
        "\n",
        "# ================================\n",
        "# 9. Process Videos\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:20]\n",
        "\n",
        "results = []\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_dir = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\n Processing {video_id}\")\n",
        "\n",
        "    # Step 1: Extract frames\n",
        "    extract_frames_1fps(video_path, frame_dir)\n",
        "\n",
        "    # Step 2: Caption each frame\n",
        "    frame_captions = []\n",
        "    for fname in sorted(os.listdir(frame_dir)):\n",
        "        if fname.endswith(\".jpg\"):\n",
        "            path = os.path.join(frame_dir, fname)\n",
        "            caption = generate_caption(path)\n",
        "            frame_captions.append(caption)\n",
        "\n",
        "    # Step 3: Summarize using BART\n",
        "    final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "    # Step 4: Metadata matching\n",
        "    meta = metadata_dict.get(video_id, {})\n",
        "    animals = meta.get(\"list_animal\", [])\n",
        "    actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "    try:\n",
        "        actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "    except:\n",
        "        actions = []\n",
        "\n",
        "    keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions\": frame_captions,\n",
        "        \"keywords\": keywords,\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 10. Save Results\n",
        "# ================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/blip2_bart_results.csv\", index=False)\n",
        "\n",
        "print(\"\\n BLIP2 + BART Evaluation Complete! Results saved to:\")\n",
        "print(\" /content/blip2_bart_results.csv\")\n"
      ],
      "metadata": {
        "id": "tzWJ_j89zbIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load results\n",
        "results_df = pd.read_csv(\"/content/blip2_bart_results.csv\")\n",
        "\n",
        "# Display key columns\n",
        "print(results_df[[\"video_id\", \"semantic_correctness_percent\", \"matched_keywords\", \"missing_keywords\"]])\n"
      ],
      "metadata": {
        "id": "aL7Omelo0qNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZq-GDuL11N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IHGfnz1R11KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PYGO8f9w11HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4fps(BLIP2+BART)"
      ],
      "metadata": {
        "id": "E3De1jvU11yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Install Dependencies\n",
        "# ================================\n",
        "# !pip install git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas -q\n",
        "\n",
        "# ================================\n",
        "# 2. Import Libraries\n",
        "# ================================\n",
        "import os, cv2, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 3. Device Setup\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4. BLIP2 Setup (Captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device, torch.float16)\n",
        "    outputs = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    return blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# ================================\n",
        "# 5. BART Setup (Summarization)\n",
        "# ================================\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def summarize_captions_bart(captions):\n",
        "    text_input = \" \".join(captions)\n",
        "    prompt = f\"Generate one concise descriptive sentence: {text_input}\"\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=25,\n",
        "        min_length=5,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).split(\".\")[0].strip() + \".\"\n",
        "\n",
        "# ================================\n",
        "# 6. Frame Extraction\n",
        "# ================================\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps / 4) if fps >= 4 else 1  # Capture every 0.25 sec\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7. Metadata Matching Function\n",
        "# ================================\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if keywords else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "# ================================\n",
        "# 8. Load Metadata\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "metadata_dict = {row[\"video_id\"]: row for _, row in meta_df.iterrows()}\n",
        "\n",
        "# ================================\n",
        "# 9. Process Videos\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:20]\n",
        "\n",
        "results = []\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_dir = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\n Processing {video_id}\")\n",
        "\n",
        "    # Step 1: Extract frames\n",
        "    extract_frames_4fps(video_path, frame_dir)\n",
        "\n",
        "    # Step 2: Caption each frame\n",
        "    frame_captions = []\n",
        "    for fname in sorted(os.listdir(frame_dir)):\n",
        "        if fname.endswith(\".jpg\"):\n",
        "            path = os.path.join(frame_dir, fname)\n",
        "            caption = generate_caption(path)\n",
        "            frame_captions.append(caption)\n",
        "\n",
        "    # Step 3: Summarize using BART\n",
        "    final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "    # Step 4: Metadata matching\n",
        "    meta = metadata_dict.get(video_id, {})\n",
        "    animals = meta.get(\"list_animal\", [])\n",
        "    actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "    try:\n",
        "        actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "    except:\n",
        "        actions = []\n",
        "\n",
        "    keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions\": frame_captions,\n",
        "        \"keywords\": keywords,\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 10. Save Results\n",
        "# ================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/blip2_bart_results.csv\", index=False)\n",
        "\n",
        "print(\"\\n BLIP2 + BART Evaluation Complete! Results saved to:\")\n",
        "print(\" /content/blip2_bart_results.csv\")\n"
      ],
      "metadata": {
        "id": "87PffU6j13TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load results\n",
        "results_df = pd.read_csv(\"/content/blip2_bart_results.csv\")\n",
        "\n",
        "# Display key columns\n",
        "print(results_df[[\"video_id\", \"semantic_correctness_percent\", \"matched_keywords\", \"missing_keywords\"]])\n"
      ],
      "metadata": {
        "id": "A2gIqNQK3gJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZQosx8I6EEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xl1GTV1a6D3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9UCgEJCf6DvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4fps(BLIP2+BART) with better prompting"
      ],
      "metadata": {
        "id": "WsQjxyBD6EiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Install Dependencies\n",
        "# ================================\n",
        "!pip install git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas -q\n",
        "\n",
        "# ================================\n",
        "# 2. Import Libraries\n",
        "# ================================\n",
        "import os, cv2, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 3. Device Setup\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4. BLIP2 Setup (Captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device, torch.float16)\n",
        "    outputs = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    return blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# ================================\n",
        "# 5. BART Setup (Summarization)\n",
        "# ================================\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def summarize_captions_bart(captions):\n",
        "    text_input = \" \".join(captions)\n",
        "    prompt = (\n",
        "        \"Summarize the following wildlife video scenes with high detail and precision. \"\n",
        "        \"Retain unique animal behaviors, actions, and surroundings: \" + text_input\n",
        "    )\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=60,\n",
        "        min_length=15,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=2.0\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6. Frame Extraction\n",
        "# ================================\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps / 4) if fps >= 4 else 1  # Capture every 0.25 sec\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7. Metadata Matching Function\n",
        "# ================================\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if keywords else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "# ================================\n",
        "# 8. Load Metadata\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "metadata_dict = {row[\"video_id\"]: row for _, row in meta_df.iterrows()}\n",
        "\n",
        "# ================================\n",
        "# 9. Process Videos\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:20]\n",
        "\n",
        "results = []\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_dir = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\n Processing {video_id}\")\n",
        "\n",
        "    # Step 1: Extract frames\n",
        "    extract_frames_4fps(video_path, frame_dir)\n",
        "\n",
        "    # Step 2: Caption each frame\n",
        "    frame_captions = []\n",
        "    for fname in sorted(os.listdir(frame_dir)):\n",
        "        if fname.endswith(\".jpg\"):\n",
        "            path = os.path.join(frame_dir, fname)\n",
        "            caption = generate_caption(path)\n",
        "            frame_captions.append(caption)\n",
        "\n",
        "    # Step 3: Summarize using BART\n",
        "    final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "    # Step 4: Metadata matching\n",
        "    meta = metadata_dict.get(video_id, {})\n",
        "    animals = meta.get(\"list_animal\", [])\n",
        "    actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "    try:\n",
        "        actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "    except:\n",
        "        actions = []\n",
        "\n",
        "    keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions\": frame_captions,\n",
        "        \"keywords\": keywords,\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 10. Save Results\n",
        "# ================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/blip2_bart_results.csv\", index=False)\n",
        "\n",
        "print(\"\\n BLIP2 + BART Evaluation Complete! Results saved to:\")\n",
        "print(\" /content/blip2_bart_results.csv\")\n"
      ],
      "metadata": {
        "id": "8FJWMASX6JfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load results\n",
        "results_df = pd.read_csv(\"/content/blip2_bart_results.csv\")\n",
        "\n",
        "# Display key columns\n",
        "print(results_df[[\"video_id\", \"semantic_correctness_percent\"]])\n"
      ],
      "metadata": {
        "id": "Lu3fRZjD8aVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8ndFJfqBQY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A set of 1000 videos"
      ],
      "metadata": {
        "id": "hoIfNjPBRgx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 0) (Optional) Installs (Colab/Notebook)\n",
        "# ================================\n",
        "# If you already have these, you can skip this cell.\n",
        "!pip install transformers timm accelerate opencv-python pandas pillow -q\n",
        "\n",
        "# ================================\n",
        "# 1) Imports\n",
        "# ================================\n",
        "import os, random, json, math, sys, gc, shutil, time\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Try to enable Colab batch download\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 2) Config\n",
        "# ================================\n",
        "VIDEO_DIR = \"/content/videos/video\"   # folder containing .mp4 files\n",
        "FRAMES_ROOT = \"/content/frames_4fps\"  # where frames will be extracted\n",
        "METADATA_XLSX = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "\n",
        "TOTAL_VIDEOS = 1000\n",
        "BATCH_SIZE = 100                     # save & download every N videos\n",
        "MAX_NEW_TOKENS_CAPTION = 50\n",
        "SUMMARY_MAX_LEN = 60\n",
        "SUMMARY_MIN_LEN = 15\n",
        "SEED = 42\n",
        "\n",
        "# If you want to keep frame extractions for debugging, set to True\n",
        "KEEP_FRAMES = False\n",
        "\n",
        "# ================================\n",
        "# 3) Reproducibility\n",
        "# ================================\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ================================\n",
        "# 4) Device\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ================================\n",
        "# 5) Load Models\n",
        "# ================================\n",
        "# BLIP-2 for per-frame captions\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "# Use bfloat16 or float16 if CUDA; fallback to float32 on CPU\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    torch_dtype=torch_dtype\n",
        ")\n",
        "if not torch.cuda.is_available():\n",
        "    blip_model = blip_model.to(device)\n",
        "\n",
        "# BART for summarization\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "# ================================\n",
        "# 6) Helper Functions\n",
        "# ================================\n",
        "def safe_makedirs(path: str):\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def extract_frames_4fps(video_path: str, out_dir: str) -> int:\n",
        "    \"\"\"Extract frames at ~4fps (every 0.25s). Returns #saved frames.\"\"\"\n",
        "    safe_makedirs(out_dir)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    if not fps or math.isnan(fps) or fps <= 0:\n",
        "        fps = 25.0  # fallback assumption\n",
        "    interval = max(int(round(fps / 4.0)), 1)\n",
        "    success, frame = cap.read()\n",
        "    count = 0\n",
        "    saved = 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(out_dir, f\"frame_{saved:04d}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            saved += 1\n",
        "        success, frame = cap.read()\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return saved\n",
        "\n",
        "def generate_caption(image_path: str) -> str:\n",
        "    \"\"\"Generate a single-frame caption using BLIP-2.\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device, dtype=torch_dtype)\n",
        "    with torch.no_grad():\n",
        "        output_ids = blip_model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS_CAPTION)\n",
        "    cap = blip_processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
        "    return cap.strip()\n",
        "\n",
        "def summarize_captions_bart(captions: List[str]) -> str:\n",
        "    \"\"\"Summarize frame-level captions into one concise description.\"\"\"\n",
        "    if not captions:\n",
        "        return \"\"\n",
        "    text_input = \" \".join(captions)\n",
        "    prompt = (\n",
        "        \"Summarize the following wildlife video scenes with high detail and precision. \"\n",
        "        \"Retain unique animal behaviors, actions, and surroundings: \"\n",
        "        + text_input\n",
        "    )\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        summary_ids = bart_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=SUMMARY_MAX_LEN,\n",
        "            min_length=SUMMARY_MIN_LEN,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3,\n",
        "            repetition_penalty=2.0\n",
        "        )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "def semantic_correctness_score(caption: str, keywords: List[str]) -> float:\n",
        "    \"\"\"Fraction of keywords present in caption (case-insensitive).\"\"\"\n",
        "    if not keywords:\n",
        "        return 0.0\n",
        "    cap = caption.lower()\n",
        "    hits = sum(1 for k in keywords if k and k.lower() in cap)\n",
        "    return hits / len([k for k in keywords if k]) if keywords else 0.0\n",
        "\n",
        "def parse_metadata_row(row: pd.Series) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Returns (animals, actions) for a given metadata row.\n",
        "    - animals: row['list_animal'], can be list or comma-separated str\n",
        "    - actions: row['list_animal_action'], can be \"[(animal, action), ...]\" or list of such pairs or comma-separated actions\n",
        "    \"\"\"\n",
        "    animals_raw = row.get(\"list_animal\", [])\n",
        "    actions_raw = row.get(\"list_animal_action\", \"\")\n",
        "\n",
        "    # Animals\n",
        "    if isinstance(animals_raw, list):\n",
        "        animals = animals_raw\n",
        "    elif isinstance(animals_raw, str):\n",
        "        # Try to split by comma if not a JSON-like list\n",
        "        animals = [a.strip() for a in animals_raw.split(\",\") if a.strip()]\n",
        "    else:\n",
        "        animals = []\n",
        "\n",
        "    # Actions\n",
        "    actions = []\n",
        "    if isinstance(actions_raw, list):\n",
        "        # could be list of tuples or list of strings\n",
        "        for item in actions_raw:\n",
        "            if isinstance(item, (list, tuple)) and len(item) >= 2:\n",
        "                actions.append(str(item[1]))\n",
        "            elif isinstance(item, str):\n",
        "                actions.append(item)\n",
        "    elif isinstance(actions_raw, str) and actions_raw.strip():\n",
        "        # Try to eval if it looks like a Python list of tuples; fall back to comma-split\n",
        "        try:\n",
        "            parsed = eval(actions_raw)\n",
        "            if isinstance(parsed, list):\n",
        "                for it in parsed:\n",
        "                    if isinstance(it, (list, tuple)) and len(it) >= 2:\n",
        "                        actions.append(str(it[1]))\n",
        "                    elif isinstance(it, str):\n",
        "                        actions.append(it)\n",
        "        except Exception:\n",
        "            actions = [a.strip() for a in actions_raw.split(\",\") if a.strip()]\n",
        "    return animals, actions\n",
        "\n",
        "def maybe_download(filepath: str):\n",
        "    \"\"\"Trigger file download in Colab; otherwise just print saved path.\"\"\"\n",
        "    if IN_COLAB:\n",
        "        try:\n",
        "            colab_files.download(filepath)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Auto-download failed for {filepath}: {e}\")\n",
        "    else:\n",
        "        print(f\"[INFO] Saved: {filepath}\")\n",
        "\n",
        "# ================================\n",
        "# 7) Load Metadata\n",
        "# ================================\n",
        "assert os.path.exists(METADATA_XLSX), f\"Metadata not found: {METADATA_XLSX}\"\n",
        "meta_df = pd.read_excel(METADATA_XLSX)\n",
        "# Build a map from video_id -> (animals, actions)\n",
        "metadata_map = {}\n",
        "for _, r in meta_df.iterrows():\n",
        "    vid = str(r.get(\"video_id\", \"\")).strip()\n",
        "    if not vid:\n",
        "        continue\n",
        "    animals, actions = parse_metadata_row(r)\n",
        "    metadata_map[vid] = (\n",
        "        [a.lower() for a in animals],\n",
        "        [a.lower() for a in actions]\n",
        "    )\n",
        "\n",
        "# ================================\n",
        "# 8) Collect & Sample Videos\n",
        "# ================================\n",
        "all_videos = sorted([str(p) for p in Path(VIDEO_DIR).glob(\"*.mp4\")])\n",
        "if not all_videos:\n",
        "    raise FileNotFoundError(f\"No .mp4 files found under {VIDEO_DIR}\")\n",
        "\n",
        "sample_n = min(TOTAL_VIDEOS, len(all_videos))\n",
        "random.shuffle(all_videos)\n",
        "video_subset = all_videos[:sample_n]\n",
        "print(f\"Found {len(all_videos)} videos. Sampling {sample_n} for processing.\")\n",
        "\n",
        "# ================================\n",
        "# 9) Main Loop (process in batches of 100)\n",
        "# ================================\n",
        "safe_makedirs(FRAMES_ROOT)\n",
        "results_master = []  # keep small dicts to optionally save one final file if desired\n",
        "\n",
        "num_batches = math.ceil(sample_n / BATCH_SIZE)\n",
        "for b in range(num_batches):\n",
        "    start = b * BATCH_SIZE\n",
        "    end = min((b + 1) * BATCH_SIZE, sample_n)\n",
        "    batch_videos = video_subset[start:end]\n",
        "    batch_rows = []\n",
        "\n",
        "    print(f\"\\n=== Processing batch {b+1}/{num_batches}: videos {start+1}–{end} ===\")\n",
        "\n",
        "    for idx, vpath in enumerate(batch_videos, start=1):\n",
        "        video_id = Path(vpath).stem\n",
        "        frame_dir = os.path.join(FRAMES_ROOT, video_id)\n",
        "        try:\n",
        "            # 1) Extract frames\n",
        "            n_frames = extract_frames_4fps(vpath, frame_dir)\n",
        "\n",
        "            # 2) Caption frames\n",
        "            frame_files = sorted([str(p) for p in Path(frame_dir).glob(\"*.jpg\")])\n",
        "            frame_captions = []\n",
        "            for f in frame_files:\n",
        "                try:\n",
        "                    cap = generate_caption(f)\n",
        "                except Exception as e:\n",
        "                    cap = \"\"  # skip faulty frame\n",
        "                frame_captions.append(cap)\n",
        "\n",
        "            # 3) Summarize\n",
        "            final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "            # 4) Semantic correctness\n",
        "            animals, actions = metadata_map.get(video_id, ([], []))\n",
        "            keywords = animals + actions\n",
        "            score = semantic_correctness_score(final_caption, keywords)\n",
        "            percent = round(score * 100.0, 1)\n",
        "\n",
        "            # Append row (ONLY what you asked for)\n",
        "            row = {\n",
        "                \"video_id\": video_id,\n",
        "                \"summarized_caption\": final_caption,\n",
        "                \"semantic_correctness_percent\": f\"{percent:.1f}%\"\n",
        "            }\n",
        "            batch_rows.append(row)\n",
        "            results_master.append(row)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log an error row so you don't lose tracking\n",
        "            err_row = {\n",
        "                \"video_id\": video_id,\n",
        "                \"summarized_caption\": \"\",\n",
        "                \"semantic_correctness_percent\": \"0.0%\",\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "            batch_rows.append(err_row)\n",
        "            results_master.append(err_row)\n",
        "        finally:\n",
        "            # Optional: free memory & (optionally) cleanup frames\n",
        "            if not KEEP_FRAMES:\n",
        "                try:\n",
        "                    shutil.rmtree(frame_dir, ignore_errors=True)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            gc.collect()\n",
        "\n",
        "    # === Save and download this batch ===\n",
        "    batch_df = pd.DataFrame(batch_rows)\n",
        "    batch_csv = f\"/content/semantic_batch_{b+1:02d}_{start+1:05d}-{end:05d}.csv\"\n",
        "    batch_df.to_csv(batch_csv, index=False)\n",
        "    print(f\"[SAVE] Batch {b+1} saved to: {batch_csv}\")\n",
        "    maybe_download(batch_csv)\n",
        "\n",
        "# ================================\n",
        "# 10) Done\n",
        "# ================================\n",
        "print(f\"\\n Finished processing {sample_n} videos.\")\n",
        "if sample_n >= 1000:\n",
        "    print(\" 1000 videos are done generating summarized captions and semantic correctness percentage.\")\n",
        "else:\n",
        "    print(\"ℹ Processed fewer than 1000 videos because the folder had fewer files.\")\n",
        "\n",
        "# (Optional) Also save a single combined CSV:\n",
        "combined_csv = \"/content/semantic_results_all.csv\"\n",
        "pd.DataFrame(results_master).to_csv(combined_csv, index=False)\n",
        "print(f\"[SAVE] Combined results saved to: {combined_csv}\")\n",
        "maybe_download(combined_csv)\n"
      ],
      "metadata": {
        "id": "nSTOyaxiBQMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rO_cc7iMoRwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "--ZqhaipReL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning on\n",
        "num_beams,early_stopping=True,no_repeat_ngram_size,length_penalty,repetition_penalty"
      ],
      "metadata": {
        "id": "CaN0Vt3ToSzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Install Dependencies\n",
        "# ================================\n",
        "!pip install git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas -q\n",
        "\n",
        "# ================================\n",
        "# 2. Import Libraries\n",
        "# ================================\n",
        "import os, cv2, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 3. Device Setup\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4. BLIP2 Setup (Captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device, torch.float16)\n",
        "    outputs = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    return blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# ================================\n",
        "# 5. BART Setup (Summarization)\n",
        "# ================================\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def summarize_captions_bart(captions):\n",
        "    text_input = \" \".join(captions)\n",
        "    prompt = (\n",
        "        \"Summarize the following wildlife video scenes with high detail and precision. \"\n",
        "        \"Retain unique animal behaviors, actions, and surroundings: \" + text_input\n",
        "    )\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=50,\n",
        "        min_length=15,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2,\n",
        "        length_penalty=2.0,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6. Frame Extraction\n",
        "# ================================\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps / 4) if fps >= 4 else 1  # Capture every 0.25 sec\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7. Metadata Matching Function\n",
        "# ================================\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if keywords else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "# ================================\n",
        "# 8. Load Metadata\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "metadata_dict = {row[\"video_id\"]: row for _, row in meta_df.iterrows()}\n",
        "\n",
        "# ================================\n",
        "# 9. Process Videos\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:20]\n",
        "\n",
        "results = []\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_dir = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\n Processing {video_id}\")\n",
        "\n",
        "    # Step 1: Extract frames\n",
        "    extract_frames_4fps(video_path, frame_dir)\n",
        "\n",
        "    # Step 2: Caption each frame\n",
        "    frame_captions = []\n",
        "    for fname in sorted(os.listdir(frame_dir)):\n",
        "        if fname.endswith(\".jpg\"):\n",
        "            path = os.path.join(frame_dir, fname)\n",
        "            caption = generate_caption(path)\n",
        "            frame_captions.append(caption)\n",
        "\n",
        "    # Step 3: Summarize using BART\n",
        "    final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "    # Step 4: Metadata matching\n",
        "    meta = metadata_dict.get(video_id, {})\n",
        "    animals = meta.get(\"list_animal\", [])\n",
        "    actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "    try:\n",
        "        actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "    except:\n",
        "        actions = []\n",
        "\n",
        "    keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions\": frame_captions,\n",
        "        \"keywords\": keywords,\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 10. Save Results\n",
        "# ================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/blip2_bart_results.csv_1\", index=False)\n",
        "\n",
        "print(\"\\n BLIP2 + BART Evaluation Complete! Results saved to:\")\n",
        "print(\" /content/blip2_bart_results.csv\")\n"
      ],
      "metadata": {
        "id": "D8CJv2N4oRkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load results\n",
        "results_df = pd.read_csv(\"/content/blip2_bart_results.csv_1\")\n",
        "\n",
        "# Display key columns\n",
        "print(results_df[[\"video_id\", \"semantic_correctness_percent\"]])\n"
      ],
      "metadata": {
        "id": "P6rEyZwEoRYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJnNP8M5oRK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Install Dependencies\n",
        "# ================================\n",
        "# !pip install git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas -q\n",
        "\n",
        "# ================================\n",
        "# 2. Import Libraries\n",
        "# ================================\n",
        "import os, cv2, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 3. Device Setup\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4. BLIP2 Setup (Captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device, torch.float16)\n",
        "    outputs = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    return blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# ================================\n",
        "# 5. BART Setup (Summarization)\n",
        "# ================================\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def summarize_captions_bart(captions):\n",
        "    text_input = \" \".join(captions)\n",
        "    prompt = (\n",
        "        \"Summarize the following wildlife video scenes with high detail and precision. \"\n",
        "        \"Retain unique animal behaviors, actions, and surroundings: \" + text_input\n",
        "    )\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=50,\n",
        "        min_length=15,\n",
        "        num_beams=6,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        length_penalty=1.0,\n",
        "        repetition_penalty=1.15\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6. Frame Extraction\n",
        "# ================================\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps / 4) if fps >= 4 else 1  # Capture every 0.25 sec\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7. Metadata Matching Function\n",
        "# ================================\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if keywords else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "# ================================\n",
        "# 8. Load Metadata\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "metadata_dict = {row[\"video_id\"]: row for _, row in meta_df.iterrows()}\n",
        "\n",
        "# ================================\n",
        "# 9. Process Videos\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:20]\n",
        "\n",
        "results = []\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_dir = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\n Processing {video_id}\")\n",
        "\n",
        "    # Step 1: Extract frames\n",
        "    extract_frames_4fps(video_path, frame_dir)\n",
        "\n",
        "    # Step 2: Caption each frame\n",
        "    frame_captions = []\n",
        "    for fname in sorted(os.listdir(frame_dir)):\n",
        "        if fname.endswith(\".jpg\"):\n",
        "            path = os.path.join(frame_dir, fname)\n",
        "            caption = generate_caption(path)\n",
        "            frame_captions.append(caption)\n",
        "\n",
        "    # Step 3: Summarize using BART\n",
        "    final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "    # Step 4: Metadata matching\n",
        "    meta = metadata_dict.get(video_id, {})\n",
        "    animals = meta.get(\"list_animal\", [])\n",
        "    actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "    try:\n",
        "        actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "    except:\n",
        "        actions = []\n",
        "\n",
        "    keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions\": frame_captions,\n",
        "        \"keywords\": keywords,\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 10. Save Results\n",
        "# ================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/blip2_bart_results.csv_2\", index=False)\n",
        "\n",
        "print(\"\\n BLIP2 + BART Evaluation Complete! Results saved to:\")\n",
        "print(\" /content/blip2_bart_results.csv_2\")\n"
      ],
      "metadata": {
        "id": "G0jqB1FSoXRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load results\n",
        "results_df = pd.read_csv(\"/content/blip2_bart_results.csv_2\")\n",
        "\n",
        "# Display key columns\n",
        "print(results_df[[\"video_id\", \"semantic_correctness_percent\"]])\n"
      ],
      "metadata": {
        "id": "8i49WcOIoWyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X9g6Mq67rLB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Install Dependencies\n",
        "# ================================\n",
        "# !pip install git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas -q\n",
        "\n",
        "# ================================\n",
        "# 2. Import Libraries\n",
        "# ================================\n",
        "import os, cv2, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 3. Device Setup\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4. BLIP2 Setup (Captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device, torch.float16)\n",
        "    outputs = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    return blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# ================================\n",
        "# 5. BART Setup (Summarization)\n",
        "# ================================\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def summarize_captions_bart(captions):\n",
        "    text_input = \" \".join(captions)\n",
        "    prompt = (\n",
        "    \"You are an expert wildlife describer. Summarize the following wildlife video text faithfully and with high precision. \"\n",
        "    \"Write one concise paragraph that mentions habitat/surroundings and lists 3–6 distinct Subject–Action–Object events. \"\n",
        "    \"Text: \" + text_input\n",
        ")\n",
        "\n",
        "\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=60,\n",
        "        min_length=15,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        length_penalty=2.0,\n",
        "        repetition_penalty=2.0\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6. Frame Extraction\n",
        "# ================================\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps / 4) if fps >= 4 else 1  # Capture every 0.25 sec\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7. Metadata Matching Function\n",
        "# ================================\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if keywords else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "# ================================\n",
        "# 8. Load Metadata\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "metadata_dict = {row[\"video_id\"]: row for _, row in meta_df.iterrows()}\n",
        "\n",
        "# ================================\n",
        "# 9. Process Videos\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:20]\n",
        "\n",
        "results = []\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_dir = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\n Processing {video_id}\")\n",
        "\n",
        "    # Step 1: Extract frames\n",
        "    extract_frames_4fps(video_path, frame_dir)\n",
        "\n",
        "    # Step 2: Caption each frame\n",
        "    frame_captions = []\n",
        "    for fname in sorted(os.listdir(frame_dir)):\n",
        "        if fname.endswith(\".jpg\"):\n",
        "            path = os.path.join(frame_dir, fname)\n",
        "            caption = generate_caption(path)\n",
        "            frame_captions.append(caption)\n",
        "\n",
        "    # Step 3: Summarize using BART\n",
        "    final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "    # Step 4: Metadata matching\n",
        "    meta = metadata_dict.get(video_id, {})\n",
        "    animals = meta.get(\"list_animal\", [])\n",
        "    actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "    try:\n",
        "        actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "    except:\n",
        "        actions = []\n",
        "\n",
        "    keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions\": frame_captions,\n",
        "        \"keywords\": keywords,\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 10. Save Results\n",
        "# ================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/blip2_bart_results.csv_5\", index=False)\n",
        "\n",
        "print(\"\\n BLIP2 + BART Evaluation Complete! Results saved to:\")\n",
        "print(\" /content/blip2_bart_results.csv_5\")\n"
      ],
      "metadata": {
        "id": "qN6cA5morMHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load results\n",
        "results_df = pd.read_csv(\"/content/blip2_bart_results.csv_4\")\n",
        "\n",
        "# Display key columns\n",
        "print(results_df[[\"video_id\", \"semantic_correctness_percent\"]])\n"
      ],
      "metadata": {
        "id": "N3q6COgJrMyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcO32D3CrORK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QJBYSKnJrOBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "InstructBLIP"
      ],
      "metadata": {
        "id": "y26ldf3aBRDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")\n"
      ],
      "metadata": {
        "id": "NpOPRslhjlMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
        "\n",
        "processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n",
        "model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\").to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "DzaEdPmSj8tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
        "\n",
        "# ========== 1. Setup ==========\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "video_id = \"LKBDONQN\"\n",
        "video_path = f\"/content/videos/video/{video_id}.mp4\"\n",
        "frame_dir = f\"/content/frames/{video_id}\"\n",
        "os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "# ========== 2. Load InstructBLIP ==========\n",
        "processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n",
        "model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\").to(device)\n",
        "\n",
        "# ========== 3. Extract Frames (4 FPS) ==========\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps / 4) if fps >= 4 else 1\n",
        "    count = saved = 0\n",
        "    success, image = vidcap.read()\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            fpath = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(fpath, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n",
        "\n",
        "print(f\"Extracting frames from {video_path} ...\")\n",
        "num_frames = extract_frames_4fps(video_path, frame_dir)\n",
        "print(f\"Extracted {num_frames} frames.\")\n",
        "\n",
        "# ========== 4. Caption Each Frame ==========\n",
        "def generate_caption_instructblip(image_path, instruction=\"Describe the animal's behavior in this image.\"):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(images=image, text=instruction, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "    return processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"\\nGenerating captions per frame...\\n\")\n",
        "frame_captions = []\n",
        "for fname in sorted(os.listdir(frame_dir)):\n",
        "    if fname.endswith(\".jpg\"):\n",
        "        fpath = os.path.join(frame_dir, fname)\n",
        "        caption = generate_caption_instructblip(fpath)\n",
        "        print(f\"{fname}: {caption}\")\n",
        "        frame_captions.append(caption)\n"
      ],
      "metadata": {
        "id": "N-oXBMkMRIV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Load BART summarization model\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def summarize_captions(captions, max_length=30, min_length=15):\n",
        "    context = \" \".join(captions)\n",
        "    prompt = \"Summarize the following wildlife observations:\\n\" + context\n",
        "\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=2.0,\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# Summarize and display result\n",
        "video_summary = summarize_captions(frame_captions)\n",
        "print(\"\\n Final Video Caption (Summary):\", video_summary)\n"
      ],
      "metadata": {
        "id": "0kR8mvAHR7M8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}