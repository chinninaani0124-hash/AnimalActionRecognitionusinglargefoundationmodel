{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCXunYUszDxD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/dataset\""
      ],
      "metadata": {
        "id": "_DUi3yDDzKKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/videos\n",
        "!tar -xvzf \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/dataset/video.tar.gz\" -C /content/videos"
      ],
      "metadata": {
        "id": "nvKEDLsPzNaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "video_dir = \"/content/videos/video\"\n",
        "video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
        "print(f\"Found {len(video_files)} video files.\")\n",
        "print(video_files[:5])  # show a few"
      ],
      "metadata": {
        "id": "BbuNG-_ozQjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "df = pd.read_excel(metadata_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "0GGRUwLAzUIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformers for BLIP\n",
        "!pip install transformers\n",
        "\n",
        "#Torch for model and inference\n",
        "!pip install torch torchvision torchaudio\n",
        "\n",
        "#Excel file reading\n",
        "!pip install openpyxl pandas\n",
        "\n",
        "#Image and video processing\n",
        "!pip install opencv-python pillow"
      ],
      "metadata": {
        "id": "G5MKWGka1bWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, json, gc, cv2, torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "1H5b2hFn2q4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLIP‑2 via HF Transformers\n",
        "blip_model_id = \"Salesforce/blip2-opt-2.7b\"   # you can swap to flan‑t5 variant if you prefer\n",
        "blip_processor = Blip2Processor.from_pretrained(blip_model_id)\n",
        "# If you're tight on VRAM, you can remove device_map and keep it on CPU. CUDA is faster.\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    blip_model_id,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None\n",
        ")\n",
        "blip_model.eval()\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_caption(image_path: str, max_new_tokens: int = 50) -> str:\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
        "    if device == \"cuda\":\n",
        "        inputs = {k: v.to(blip_model.device, dtype=torch.float16 if v.dtype.is_floating_point else None)\n",
        "                  for k, v in inputs.items()}\n",
        "    outputs = blip_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    caption = blip_processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
        "    return caption\n"
      ],
      "metadata": {
        "id": "XeV22RiG2wjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bart_model_id = \"facebook/bart-large-cnn\"\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_id)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_id).to(device)\n",
        "bart_model.eval()\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_captions_bart(captions, max_len=60, min_len=15) -> str:\n",
        "    text_input = \" \".join(captions)\n",
        "    prompt = (\"Summarize the following wildlife video scenes with high detail and precision. \"\n",
        "              \"Retain unique animal behaviors, actions, and surroundings: \" + text_input)\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_len,\n",
        "        min_length=min_len,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=2.0\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "id": "QdBjibMM27a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 0\n",
        "    interval = int(fps // 4) if fps and fps >= 4 else 1\n",
        "    success, frame = cap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            saved += 1\n",
        "        success, frame = cap.read()\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return saved\n"
      ],
      "metadata": {
        "id": "63xBewpv2_aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_correctness_score(caption, keywords):\n",
        "    cap = caption.lower()\n",
        "    total = len(keywords)\n",
        "    hits = sum(1 for kw in keywords if kw.lower() in cap)\n",
        "    return (hits / total) if total else 0.0\n"
      ],
      "metadata": {
        "id": "u_G1z1up3Fs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "video_dir = \"/content/videos/video\"\n",
        "frames_root = \"/content/frames\"\n",
        "save_csv = \"/content/blip2_bart_results.csv\"\n",
        "\n",
        "# Load metadata: video_id | list_animal | list_animal_action\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "# Normalize into a dict of plain Python structures\n",
        "metadata = {}\n",
        "for _, r in meta_df.iterrows():\n",
        "    vid = str(r[\"video_id\"])\n",
        "    animals = r.get(\"list_animal\", [])\n",
        "    if isinstance(animals, str):\n",
        "        # If CSV stored JSON-like array as string, try to eval safely\n",
        "        try:\n",
        "            animals = eval(animals)\n",
        "        except Exception:\n",
        "            animals = [animals] if animals else []\n",
        "    actions_raw = r.get(\"list_animal_action\", \"\")\n",
        "    actions = []\n",
        "    if isinstance(actions_raw, str) and actions_raw.strip():\n",
        "        try:\n",
        "            # expecting something like [(animal, action), ...]\n",
        "            actions = [act for (_, act) in eval(actions_raw)]\n",
        "        except Exception:\n",
        "            pass\n",
        "    metadata[vid] = {\n",
        "        \"animals\": animals if isinstance(animals, list) else [animals],\n",
        "        \"actions\": actions\n",
        "    }\n",
        "\n",
        "# Choose your set (here: first 20 mp4s just as a smoke test)\n",
        "all_videos = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:20]\n",
        "\n",
        "rows = []\n",
        "for v in all_videos:\n",
        "    video_id = Path(v).stem\n",
        "    vpath = os.path.join(video_dir, v)\n",
        "    fdir = os.path.join(frames_root, video_id)\n",
        "    print(f\"\\nProcessing {video_id}\")\n",
        "\n",
        "    # 1) Frames\n",
        "    _ = extract_frames_4fps(vpath, fdir)\n",
        "\n",
        "    # 2) Caption each frame (you can subsample if many)\n",
        "    frame_files = [os.path.join(fdir, f) for f in sorted(os.listdir(fdir)) if f.endswith(\".jpg\")]\n",
        "    frame_captions = []\n",
        "    for fimg in frame_files:\n",
        "        try:\n",
        "            frame_captions.append(generate_caption(fimg))\n",
        "        except Exception as e:\n",
        "            # be resilient; skip bad frames\n",
        "            print(f\"  warn: caption failed on {Path(fimg).name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not frame_captions:\n",
        "        print(\"  warn: no captions, skipping\")\n",
        "        continue\n",
        "\n",
        "    # 3) Summarize\n",
        "    final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "    # 4) Simple semantic score against metadata keywords\n",
        "    meta = metadata.get(video_id, {\"animals\": [], \"actions\": []})\n",
        "    keywords = [*(meta[\"animals\"] or []), *(meta[\"actions\"] or [])]\n",
        "    sem_score = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    rows.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"semantic_correctness_percent\": f\"{sem_score*100:.1f}%\",\n",
        "        \"num_frames\": len(frame_files)\n",
        "    })\n",
        "\n",
        "# Save\n",
        "out_df = pd.DataFrame(rows)\n",
        "out_df.to_csv(save_csv, index=False)\n",
        "print(\"\\nSaved:\", save_csv)\n"
      ],
      "metadata": {
        "id": "4CXYi6M93R1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# BLIP2 → BART scalable pipeline\n",
        "# Prints final captions in Colab output\n",
        "# Saves JSONL prompts as {video_id, prompt, semantic_correctness_percent}\n",
        "# Also saves full CSV/JSONL with details\n",
        "# ================================\n",
        "\n",
        "# --- (If running in a fresh Colab) you may need installs ---\n",
        "# !pip install git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas -q\n",
        "\n",
        "import os, cv2, math, json, random, ast, gc, time, re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 1) Config\n",
        "# ================================\n",
        "VIDEO_DIR = \"/content/videos/video\"               # directory with ~30k .mp4\n",
        "METADATA_XLSX = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "\n",
        "OUTPUT_DIR = \"/content/ak_blip2_bart_results\"\n",
        "CSV_PATH = f\"{OUTPUT_DIR}/results_1000.csv\"           # detailed results\n",
        "JSONL_PATH = f\"{OUTPUT_DIR}/results_1000.jsonl\"        # detailed results (JSONL)\n",
        "PROMPTS_JSONL = f\"{OUTPUT_DIR}/prompts_1000.jsonl\"     # *** compact prompts JSONL ***\n",
        "CHECKPOINT_PATH = f\"{OUTPUT_DIR}/completed_ids.txt\"\n",
        "LOG_PATH = f\"{OUTPUT_DIR}/run.log\"\n",
        "\n",
        "NUM_VIDEOS = 1000\n",
        "RANDOM_SEED = 42\n",
        "MAX_FRAMES_PER_VIDEO = 64\n",
        "CAPTION_BATCH_SIZE = 8\n",
        "TARGET_FPS = 4\n",
        "\n",
        "# Generation knobs (speed vs quality)\n",
        "BLIP2_MAX_NEW_TOKENS = 40\n",
        "BLIP2_NUM_BEAMS = 1\n",
        "\n",
        "BART_MAX_LEN = 60\n",
        "BART_MIN_LEN = 15\n",
        "BART_NUM_BEAMS = 1\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def log(msg: str):\n",
        "    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    with open(LOG_PATH, \"a\") as f:\n",
        "        f.write(f\"[{ts}] {msg}\\n\")\n",
        "    print(msg)\n",
        "\n",
        "# ================================\n",
        "# 2) Load models once\n",
        "# ================================\n",
        "log(\"Loading BLIP2 + BART models…\")\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(DEVICE)\n",
        "\n",
        "# ================================\n",
        "# 3) Helpers\n",
        "# ================================\n",
        "\n",
        "def list_mp4s(dir_path: str) -> List[str]:\n",
        "    return sorted([f for f in os.listdir(dir_path) if f.lower().endswith(\".mp4\")])\n",
        "\n",
        "\n",
        "def read_completed_ids(path: str) -> set:\n",
        "    if not os.path.exists(path):\n",
        "        return set()\n",
        "    with open(path, \"r\") as f:\n",
        "        return set(line.strip() for line in f if line.strip())\n",
        "\n",
        "\n",
        "def append_completed_id(path: str, vid: str):\n",
        "    with open(path, \"a\") as f:\n",
        "        f.write(vid + \"\\n\")\n",
        "\n",
        "\n",
        "def uniform_indices(total_frames: int, max_count: int) -> List[int]:\n",
        "    if total_frames <= 0:\n",
        "        return []\n",
        "    if total_frames <= max_count:\n",
        "        return list(range(total_frames))\n",
        "    step = total_frames / float(max_count)\n",
        "    return [int(i * step) for i in range(max_count)]\n",
        "\n",
        "\n",
        "def target_step_from_fps(fps: float, target_fps: float = TARGET_FPS) -> int:\n",
        "    if fps <= 0:\n",
        "        return 1\n",
        "    step = max(1, int(round(fps / target_fps)))\n",
        "    return step\n",
        "\n",
        "\n",
        "def sample_frame_indices(cap: cv2.VideoCapture, max_frames: int) -> List[int]:\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    if fps and fps >= TARGET_FPS:\n",
        "        step = target_step_from_fps(fps, TARGET_FPS)\n",
        "        candidates = list(range(0, total_frames, step))\n",
        "        if len(candidates) > max_frames:\n",
        "            # downsample uniformly over candidates\n",
        "            idxs = uniform_indices(len(candidates), max_frames)\n",
        "            return [candidates[i] for i in idxs]\n",
        "        return candidates[:max_frames]\n",
        "    else:\n",
        "        return uniform_indices(total_frames, max_frames)\n",
        "\n",
        "\n",
        "def frames_to_pil(cap: cv2.VideoCapture, indices: List[int]) -> List[Image.Image]:\n",
        "    imgs = []\n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            continue\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        imgs.append(Image.fromarray(frame_rgb))\n",
        "    return imgs\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def caption_images_blip2(images: List[Image.Image]) -> List[str]:\n",
        "    captions = []\n",
        "    for i in range(0, len(images), CAPTION_BATCH_SIZE):\n",
        "        batch_imgs = images[i:i+CAPTION_BATCH_SIZE]\n",
        "        inputs = blip_processor(images=batch_imgs, return_tensors=\"pt\").to(blip_model.device, torch.float16)\n",
        "        out = blip_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=BLIP2_MAX_NEW_TOKENS,\n",
        "            num_beams=BLIP2_NUM_BEAMS\n",
        "        )\n",
        "        batch_caps = blip_processor.batch_decode(out, skip_special_tokens=True)\n",
        "        captions.extend([c.strip() for c in batch_caps])\n",
        "        del inputs, out\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    return captions\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_captions_bart(captions: List[str]) -> str:\n",
        "    text_input = \" \".join(captions)\n",
        "    prompt = (\n",
        "        \"Summarize the following wildlife video scenes with high detail and precision. \"\n",
        "        \"Retain unique animal behaviors, actions, and surroundings: \" + text_input\n",
        "    )\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
        "    # NOTE: do not pass early_stopping/length_penalty (ignored in recent HF)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=BART_MAX_LEN,\n",
        "        min_length=BART_MIN_LEN,\n",
        "        num_beams=BART_NUM_BEAMS,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=2.0,\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# ---- Fuzzy-ish keyword matching to reduce false 0% ----\n",
        "_non_alnum = re.compile(r\"[^a-z0-9\\s]\")\n",
        "_multispace = re.compile(r\"\\s+\")\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = _non_alnum.sub(\" \", s)\n",
        "    s = _multispace.sub(\" \", s).strip()\n",
        "    # naive singularization\n",
        "    if s.endswith(\"s\") and len(s) > 3:\n",
        "        s = s[:-1]\n",
        "    return s\n",
        "\n",
        "\n",
        "def semantic_correctness_score(caption: str, keywords: List[str]) -> Tuple[float | None, List[str], List[str]]:\n",
        "    kws = [kw for kw in keywords if isinstance(kw, str) and kw.strip()]\n",
        "    if not kws:\n",
        "        return None, [], []  # N/A — no ground truth\n",
        "    cap = \" \" + _norm(caption) + \" \"\n",
        "    matched, missing = [], []\n",
        "    for kw in kws:\n",
        "        k = _norm(kw)\n",
        "        variants = {k}\n",
        "        # a couple of common short forms\n",
        "        if \"chimpanzee\" in k: variants.add(k.replace(\"chimpanzee\", \"chimp\"))\n",
        "        if \"crocodile\" in k: variants.add(k.replace(\"crocodile\", \"croc\"))\n",
        "        hit = any((\" \" + v + \" \") in cap for v in variants)\n",
        "        (matched if hit else missing).append(kw)\n",
        "    score = len(matched) / len(kws)\n",
        "    return score, matched, missing\n",
        "\n",
        "\n",
        "def safe_parse_list(value) -> List[str]:\n",
        "    if isinstance(value, list):\n",
        "        return value\n",
        "    if not isinstance(value, str) or not value.strip():\n",
        "        return []\n",
        "    try:\n",
        "        parsed = ast.literal_eval(value)\n",
        "        return parsed if isinstance(parsed, list) else []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "\n",
        "def safe_parse_actions(value) -> List[str]:\n",
        "    if not isinstance(value, str) or not value.strip():\n",
        "        return []\n",
        "    try:\n",
        "        parsed = ast.literal_eval(value)\n",
        "        actions = []\n",
        "        if isinstance(parsed, list):\n",
        "            for item in parsed:\n",
        "                if isinstance(item, (tuple, list)) and len(item) >= 2:\n",
        "                    actions.append(str(item[1]))\n",
        "                elif isinstance(item, str):\n",
        "                    actions.append(item)\n",
        "        return actions\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "\n",
        "def write_jsonl(path: str, record: Dict[str, Any]):\n",
        "    with open(path, \"a\") as f:\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# ================================\n",
        "# 4) Load metadata\n",
        "# ================================\n",
        "log(\"Loading metadata…\")\n",
        "meta_df = pd.read_excel(METADATA_XLSX)\n",
        "if \"video_id\" not in meta_df.columns:\n",
        "    raise ValueError(\"Expected 'video_id' column in metadata.\")\n",
        "meta_df = meta_df.set_index(\"video_id\", drop=False)\n",
        "\n",
        "# ================================\n",
        "# 5) Choose random 1000 (resumable)\n",
        "# ================================\n",
        "all_files = list_mp4s(VIDEO_DIR)\n",
        "if len(all_files) < NUM_VIDEOS:\n",
        "    raise ValueError(f\"Found only {len(all_files)} MP4s, need {NUM_VIDEOS}.\")\n",
        "random.seed(RANDOM_SEED)\n",
        "sampled_files = random.sample(all_files, NUM_VIDEOS)\n",
        "\n",
        "completed = read_completed_ids(CHECKPOINT_PATH)\n",
        "remaining = [f for f in sampled_files if Path(f).stem not in completed]\n",
        "log(f\"{len(completed)} already completed, {len(remaining)} remaining.\")\n",
        "\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    pd.DataFrame(columns=[\n",
        "        \"video_id\",\"final_caption\",\"frame_captions\",\n",
        "        \"keywords\",\"matched_keywords\",\"missing_keywords\",\n",
        "        \"semantic_correctness_percent\"\n",
        "    ]).to_csv(CSV_PATH, index=False)\n",
        "\n",
        "# ================================\n",
        "# 6) Main loop — PRINT captions + SAVE prompts\n",
        "# ================================\n",
        "for idx, video_file in enumerate(remaining, 1):\n",
        "    video_id = Path(video_file).stem\n",
        "    video_path = os.path.join(VIDEO_DIR, video_file)\n",
        "    log(f\"[{idx}/{len(remaining)}] Processing {video_id}\")\n",
        "\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            log(f\"  !! Could not open {video_path}, skipping.\")\n",
        "            append_completed_id(CHECKPOINT_PATH, video_id)\n",
        "            continue\n",
        "\n",
        "        indices = sample_frame_indices(cap, MAX_FRAMES_PER_VIDEO)\n",
        "        if not indices:\n",
        "            log(\"  !! No frames found, skipping.\")\n",
        "            cap.release()\n",
        "            append_completed_id(CHECKPOINT_PATH, video_id)\n",
        "            continue\n",
        "\n",
        "        pil_images = frames_to_pil(cap, indices)\n",
        "        cap.release()\n",
        "        if not pil_images:\n",
        "            log(\"  !! Failed reading frames, skipping.\")\n",
        "            append_completed_id(CHECKPOINT_PATH, video_id)\n",
        "            continue\n",
        "\n",
        "        # ---- Caption frames (GPU batches) ----\n",
        "        frame_captions = caption_images_blip2(pil_images)\n",
        "\n",
        "        # ---- Summarize ----\n",
        "        final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "        # ---- Metadata ----\n",
        "        animals, actions = [], []\n",
        "        if video_id in meta_df.index:\n",
        "            row = meta_df.loc[video_id]\n",
        "            animals = safe_parse_list(row.get(\"list_animal\", []))\n",
        "            actions = safe_parse_actions(row.get(\"list_animal_action\", \"\"))\n",
        "        else:\n",
        "            log(f\"  !! No metadata for {video_id}\")\n",
        "\n",
        "        keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "        score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "        score_fmt = \"N/A\" if score is None else f\"{score*100:.1f}%\"\n",
        "\n",
        "        # ---- PRINT to Colab output (your request) ----\n",
        "        print(\"\\n>>>\", video_id)\n",
        "        print(\"Caption:\", final_caption)\n",
        "        if frame_captions:\n",
        "            print(\"First 5 frame captions:\")\n",
        "            for c in frame_captions[:5]:\n",
        "                print(\" -\", c)\n",
        "        print(\"Semantic correctness:\", score_fmt)\n",
        "        if score is not None:\n",
        "            print(\"Matched:\", matched)\n",
        "            print(\"Missing:\", missing)\n",
        "\n",
        "        # ---- Save detailed record ----\n",
        "        record = {\n",
        "            \"video_id\": video_id,\n",
        "            \"final_caption\": final_caption,\n",
        "            \"frame_captions\": frame_captions,\n",
        "            \"keywords\": keywords,\n",
        "            \"matched_keywords\": matched,\n",
        "            \"missing_keywords\": missing,\n",
        "            \"semantic_correctness_percent\": score_fmt\n",
        "        }\n",
        "        write_jsonl(JSONL_PATH, record)\n",
        "        pd.DataFrame([record]).to_csv(CSV_PATH, mode=\"a\", header=False, index=False)\n",
        "\n",
        "        # ---- Save compact prompts JSONL (your requested format) ----\n",
        "        prompt_rec = {\n",
        "            \"video_id\": video_id,\n",
        "            \"prompt\": final_caption,\n",
        "            \"semantic_correctness_percent\": score_fmt,\n",
        "        }\n",
        "        write_jsonl(PROMPTS_JSONL, prompt_rec)\n",
        "\n",
        "        append_completed_id(CHECKPOINT_PATH, video_id)\n",
        "\n",
        "        # ---- Housekeeping ----\n",
        "        del pil_images, frame_captions\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        log(f\"  ✓ Done {video_id} | score={score_fmt}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"  !! Error on {video_id}: {repr(e)}\")\n",
        "        append_completed_id(CHECKPOINT_PATH, video_id)\n",
        "        continue\n",
        "\n",
        "log(\"\\nBLIP2 + BART 1000-video run complete.\")\n",
        "log(f\"CSV (detailed): {CSV_PATH}\")\n",
        "log(f\"JSONL (detailed): {JSONL_PATH}\")\n",
        "log(f\"JSONL (prompts): {PROMPTS_JSONL}\")\n",
        "log(f\"Completed IDs list: {CHECKPOINT_PATH}\")\n"
      ],
      "metadata": {
        "id": "UQRfzwkj3Vi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3e2GRNkDhb7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EAav6F0Chb3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4Nv8N67hb0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First 1000 videos(BLIP2+BART+4fps+prompt)"
      ],
      "metadata": {
        "id": "NWZGmeNbPpc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Install Dependencies\n",
        "# ================================\n",
        "!pip install git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas -q\n",
        "\n",
        "# ================================\n",
        "# 2. Import Libraries\n",
        "# ================================\n",
        "import os, cv2, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 3. Device Setup\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4. BLIP2 Setup (Captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_model.device, torch.float16)\n",
        "    outputs = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    return blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# ================================\n",
        "# 5. BART Setup (Summarization)\n",
        "# ================================\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def summarize_captions_bart(captions):\n",
        "    text_input = \" \".join(captions)\n",
        "    prompt = (\n",
        "        \"Summarize the following wildlife video scenes with high detail and precision. \"\n",
        "        \"Retain unique animal behaviors, actions, and surroundings: \" + text_input\n",
        "    )\n",
        "    inputs = bart_tokenizer([prompt], return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=60,\n",
        "        min_length=15,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=2.0\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6. Frame Extraction\n",
        "# ================================\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps / 4) if fps >= 4 else 1  # Capture every 0.25 sec\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7. Metadata Matching Function\n",
        "# ================================\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption = caption.lower()\n",
        "    matched = [kw for kw in keywords if kw.lower() in caption]\n",
        "    missing = [kw for kw in keywords if kw.lower() not in caption]\n",
        "    score = len(matched) / len(keywords) if keywords else 0\n",
        "    return score, matched, missing\n",
        "\n",
        "# ================================\n",
        "# 8. Load Metadata\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "metadata_dict = {row[\"video_id\"]: row for _, row in meta_df.iterrows()}\n",
        "\n",
        "# ================================\n",
        "# 9. Process Videos\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:1000]\n",
        "\n",
        "results = []\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_dir = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\n Processing {video_id}\")\n",
        "\n",
        "    # Step 1: Extract frames\n",
        "    extract_frames_4fps(video_path, frame_dir)\n",
        "\n",
        "    # Step 2: Caption each frame\n",
        "    frame_captions = []\n",
        "    for fname in sorted(os.listdir(frame_dir)):\n",
        "        if fname.endswith(\".jpg\"):\n",
        "            path = os.path.join(frame_dir, fname)\n",
        "            caption = generate_caption(path)\n",
        "            frame_captions.append(caption)\n",
        "\n",
        "    # Step 3: Summarize using BART\n",
        "    final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "    # Step 4: Metadata matching\n",
        "    meta = metadata_dict.get(video_id, {})\n",
        "    animals = meta.get(\"list_animal\", [])\n",
        "    actions_raw = meta.get(\"list_animal_action\", \"\")\n",
        "    try:\n",
        "        actions = [act for (_, act) in eval(actions_raw)] if isinstance(actions_raw, str) else []\n",
        "    except:\n",
        "        actions = []\n",
        "\n",
        "    keywords = [a.lower() for a in animals] + [a.lower() for a in actions]\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions\": frame_captions,\n",
        "        \"keywords\": keywords,\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 10. Save Results\n",
        "# ================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/blip2_bart_results.csv\", index=False)\n",
        "\n",
        "print(\"\\n BLIP2 + BART Evaluation Complete! Results saved to:\")\n",
        "print(\" /content/blip2_bart_results.csv\")\n"
      ],
      "metadata": {
        "id": "y_K5tilJhbxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load results\n",
        "results_df = pd.read_csv(\"/content/blip2_bart_results.csv\")\n",
        "\n",
        "# Display key columns\n",
        "print(results_df[[\"video_id\", \"semantic_correctness_percent\"]])"
      ],
      "metadata": {
        "id": "BdI_V4WJKoB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMkfHy5m9Vnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ULZ_1pZz9Vj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "57ujRpDh9VgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without prompt(BLIP2+BART+4fps)"
      ],
      "metadata": {
        "id": "Q2c75-MZ9YON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Install Dependencies\n",
        "# ================================\n",
        "!pip install -q git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas\n",
        "\n",
        "# ================================\n",
        "# 2. Import Libraries\n",
        "# ================================\n",
        "import os, cv2, torch, ast\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 3. Device Setup\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4. BLIP2 Setup (Captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",                # lets Accelerate place sharded weights\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    # IMPORTANT: send tensors to the SAME device/dtype as the model\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device=blip_model.device, dtype=blip_model.dtype) for k, v in inputs.items()}\n",
        "    outputs = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    caption = blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    return caption.strip()\n",
        "\n",
        "# ================================\n",
        "# 5. BART Setup (Summarization)\n",
        "# ================================\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_captions_bart(captions, max_input_tokens=1024, max_summary_tokens=60):\n",
        "    \"\"\"\n",
        "    Summarize ONLY the concatenated captions (no instruction/prompt string).\n",
        "    \"\"\"\n",
        "    # Guard: if no captions were produced, return empty string\n",
        "    if not captions:\n",
        "        return \"\"\n",
        "\n",
        "    # Join captions with newlines to give the encoder separable sentences\n",
        "    text_input = \"\\n\".join([c for c in captions if isinstance(c, str) and c.strip()])\n",
        "    if not text_input.strip():\n",
        "        return \"\"\n",
        "\n",
        "    # Tokenize with truncation to BART's 1024-token encoder limit\n",
        "    inputs = bart_tokenizer(\n",
        "        text_input,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_input_tokens,\n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    summary_ids = bart_model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs.get(\"attention_mask\", None),\n",
        "        max_length=max_summary_tokens,\n",
        "        min_length=15,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        length_penalty=1.0\n",
        "    )\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# ================================\n",
        "# 6. Frame Extraction\n",
        "# ================================\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    if not vidcap.isOpened():\n",
        "        print(f\"  ! Could not open video: {video_path}\")\n",
        "        return 0\n",
        "\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    # Capture roughly every 0.25 sec; if FPS invalid/low, fall back to interval 1\n",
        "    try:\n",
        "        interval = int(max(1, round((fps if fps and fps > 0 else 4) / 4)))\n",
        "    except:\n",
        "        interval = 1\n",
        "\n",
        "    success, image = vidcap.read()\n",
        "    count, saved = 0, 0\n",
        "    while success:\n",
        "        if count % interval == 0:\n",
        "            frame_path = os.path.join(output_folder, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(frame_path, image)\n",
        "            saved += 1\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7. Metadata Matching Function\n",
        "# ================================\n",
        "def semantic_correctness_score(caption, keywords):\n",
        "    caption_l = (caption or \"\").lower()\n",
        "    kws = [kw.lower() for kw in (keywords or []) if isinstance(kw, str)]\n",
        "    matched = [kw for kw in kws if kw in caption_l]\n",
        "    missing = [kw for kw in kws if kw not in caption_l]\n",
        "    score = (len(matched) / len(kws)) if kws else 0.0\n",
        "    return score, matched, missing\n",
        "\n",
        "# ================================\n",
        "# 8. Load Metadata\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "\n",
        "# Build a dict: video_id -> dict with parsed animals/actions\n",
        "metadata_dict = {}\n",
        "for _, row in meta_df.iterrows():\n",
        "    vid = row.get(\"video_id\")\n",
        "    animals_raw = row.get(\"list_animal\", [])\n",
        "    actions_raw = row.get(\"list_animal_action\", \"\")\n",
        "\n",
        "    # Parse animals: handle lists stored as strings like \"['lion', 'zebra']\"\n",
        "    if isinstance(animals_raw, str):\n",
        "        try:\n",
        "            animals = ast.literal_eval(animals_raw)\n",
        "            if not isinstance(animals, list):\n",
        "                animals = [str(animals)]\n",
        "        except Exception:\n",
        "            # fall back: split on commas\n",
        "            animals = [a.strip() for a in animals_raw.split(\",\") if a.strip()]\n",
        "    elif isinstance(animals_raw, list):\n",
        "        animals = animals_raw\n",
        "    else:\n",
        "        animals = []\n",
        "\n",
        "    # Parse actions: often like \"[('lion','running'),('zebra','grazing')]\"\n",
        "    actions = []\n",
        "    if isinstance(actions_raw, str) and actions_raw.strip():\n",
        "        try:\n",
        "            parsed = ast.literal_eval(actions_raw)\n",
        "            # Accept list of tuples or list of strings\n",
        "            if isinstance(parsed, list):\n",
        "                for item in parsed:\n",
        "                    if isinstance(item, (list, tuple)) and len(item) >= 2:\n",
        "                        actions.append(str(item[1]))\n",
        "                    elif isinstance(item, str):\n",
        "                        actions.append(item)\n",
        "        except Exception:\n",
        "            # try simple comma split as a fallback\n",
        "            actions = [a.strip() for a in actions_raw.split(\",\") if a.strip()]\n",
        "\n",
        "    metadata_dict[vid] = {\n",
        "        \"animals\": animals,\n",
        "        \"actions\": actions\n",
        "    }\n",
        "\n",
        "# ================================\n",
        "# 9. Process Videos\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:1000]\n",
        "\n",
        "results = []\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_dir = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\n Processing {video_id}\")\n",
        "\n",
        "    # Step 1: Extract frames\n",
        "    n_frames = extract_frames_4fps(video_path, frame_dir)\n",
        "    if n_frames == 0:\n",
        "        print(\"  ! No frames extracted; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Step 2: Caption each frame\n",
        "    frame_captions = []\n",
        "    frame_names = sorted([fn for fn in os.listdir(frame_dir) if fn.endswith(\".jpg\")])\n",
        "    for idx, fname in enumerate(frame_names, start=1):\n",
        "        path = os.path.join(frame_dir, fname)\n",
        "        caption = generate_caption(path)\n",
        "        frame_captions.append(caption)\n",
        "        if idx % 20 == 0:\n",
        "            print(f\"  - Captioned {idx}/{len(frame_names)} frames\")\n",
        "\n",
        "    # Step 3: Summarize using BART (no instruction string!)\n",
        "    final_caption = summarize_captions_bart(frame_captions)\n",
        "\n",
        "    # Step 4: Metadata matching\n",
        "    meta = metadata_dict.get(video_id, {\"animals\": [], \"actions\": []})\n",
        "    animals = meta.get(\"animals\", [])\n",
        "    actions = meta.get(\"actions\", [])\n",
        "    keywords = [a for a in animals] + [a for a in actions]\n",
        "\n",
        "    score, matched, missing = semantic_correctness_score(final_caption, keywords)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions\": frame_captions,\n",
        "        \"keywords\": [k.lower() for k in keywords],\n",
        "        \"matched_keywords\": matched,\n",
        "        \"missing_keywords\": missing,\n",
        "        \"semantic_correctness_percent\": f\"{score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 10. Save Results\n",
        "# ================================\n",
        "results_df = pd.DataFrame(results)\n",
        "out_path = \"/content/blip2_bart_results.csv\"\n",
        "results_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"\\n BLIP2 + BART Evaluation Complete! Results saved to:\")\n",
        "print(f\" {out_path}\")\n"
      ],
      "metadata": {
        "id": "d6hwZwzw9WNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wyM_GcUPe2rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5wihps55e2nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fb_kRpyye2iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLAN-T5 summarizer+instruction prompt"
      ],
      "metadata": {
        "id": "RAtYJ6Exe7Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1) Install dependencies\n",
        "# ================================\n",
        "!pip install -q git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas nltk\n",
        "\n",
        "# ================================\n",
        "# 2) Imports\n",
        "# ================================\n",
        "import os, re, cv2, ast, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration,\n",
        "    T5Tokenizer, T5ForConditionalGeneration\n",
        ")\n",
        "\n",
        "# Lemmatization for robust scoring\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# ================================\n",
        "# 3) Device\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4) BLIP2 (frame captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        ")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device=blip_model.device, dtype=blip_model.dtype) for k, v in inputs.items()}\n",
        "    out = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    cap = blip_processor.batch_decode(out, skip_special_tokens=True)[0]\n",
        "    return cap.strip()\n",
        "\n",
        "# ================================\n",
        "# 5) Summarizers: FLAN (default) & BART\n",
        "# ================================\n",
        "flan_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "flan_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
        "\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def curate_captions(caps, max_caps=80, min_chars=8):\n",
        "    \"\"\"Deduplicate exact repeats, basic quality filter, uniformly sample to max_caps.\"\"\"\n",
        "    caps = [c.strip() for c in caps if isinstance(c, str) and len(c.strip()) >= min_chars]\n",
        "    seen, uniq = set(), []\n",
        "    for c in caps:\n",
        "        if c not in seen:\n",
        "            seen.add(c); uniq.append(c)\n",
        "    if len(uniq) <= max_caps:\n",
        "        return uniq\n",
        "    # uniform sampling by index\n",
        "    idxs = [round(i) for i in [j*(len(uniq)-1)/(max_caps-1) for j in range(max_caps)]]\n",
        "    return [uniq[i] for i in idxs]\n",
        "\n",
        "# --- Metadata keyword normalization (maps metadata → caption-friendly terms)\n",
        "SPECIES_MAP = {\n",
        "    # birds / common wildlife seen in your misses\n",
        "    \"eurasian wren bird\": \"wren\",\n",
        "    \"ardea alba egret\": \"egret\",\n",
        "    \"grey heron\": \"heron\",\n",
        "    \"black-winged stilt\": \"stilt\",\n",
        "    \"yellowhammer\": \"bird\",\n",
        "    \"tit bird\": \"tit\",\n",
        "    \"mallard duck\": \"duck\",\n",
        "    \"common crane\": \"crane\",\n",
        "    \"black mamba\": \"snake\",\n",
        "    # mammals genericization\n",
        "    \"mongoose\": \"mongoose\",\n",
        "    # plurals often used by BLIP -> keep generic\n",
        "}\n",
        "\n",
        "ACTION_MAP = {\n",
        "    \"keeping still\": \"standing still\",\n",
        "    \"attending\": \"looking\",\n",
        "    \"sensing\": \"looking\",         # sometimes “sniffing”/“listening”, but “looking” is safer\n",
        "    \"chirping\": \"singing\",\n",
        "    \"flapping\": \"flapping\",\n",
        "    \"landing\": \"landing\",\n",
        "    \"preening\": \"preening\",\n",
        "    \"walking\": \"walking\",\n",
        "    \"moving\": \"moving\",\n",
        "    \"eating\": \"eating\",\n",
        "    \"jumping\": \"jumping\",\n",
        "    \"attacking\": \"attacking\",\n",
        "    \"flying\": \"flying\",\n",
        "    \"shaking head\": \"shaking head\",\n",
        "    \"singing nightingale\": \"nightingale singing\"\n",
        "}\n",
        "\n",
        "def normalize_meta_keywords(animals, actions):\n",
        "    def norm_list(lst, mp):\n",
        "        out = []\n",
        "        for x in (lst or []):\n",
        "            x_l = str(x).lower()\n",
        "            out.append(mp.get(x_l, x_l))\n",
        "        return out\n",
        "    return norm_list(animals, SPECIES_MAP), norm_list(actions, ACTION_MAP)\n",
        "\n",
        "def pick_keywords_to_encourage(frame_captions, meta_keywords, max_animals=5, max_actions=3):\n",
        "    \"\"\"Keep only keys that actually appear (or their normalized forms) in captions to avoid hallucination.\"\"\"\n",
        "    text = \" \".join(frame_captions).lower()\n",
        "    animals, actions = [], []\n",
        "    for kw in meta_keywords:\n",
        "        kw_l = kw.lower()\n",
        "        if kw_l in text:\n",
        "            # crude verb vs noun heuristic\n",
        "            if any(kw_l.endswith(s) for s in (\"ing\",\"ed\")) or \" \" in kw_l:\n",
        "                actions.append(kw_l)\n",
        "            else:\n",
        "                animals.append(kw_l)\n",
        "    return animals[:max_animals] + actions[:max_actions]\n",
        "\n",
        "def build_force_ids(tokenizer, words):\n",
        "    fw = []\n",
        "    for w in words:\n",
        "        ids = tokenizer(w, add_special_tokens=False).input_ids\n",
        "        if ids:\n",
        "            fw.append(ids)\n",
        "    return fw if fw else None\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_with_flan(frame_captions, encourage_words=None, max_new_tokens=80):\n",
        "    caps = curate_captions(frame_captions, max_caps=80)\n",
        "    if not caps: return \"\"\n",
        "    # Instruction works for FLAN; include “Ensure to mention …” softly\n",
        "    prompt = \"Summarize the wildlife video in one detailed, factual sentence mentioning all animals and their actions.\\n\"\n",
        "    if encourage_words:\n",
        "        prompt += \"Ensure to mention: \" + \", \".join(encourage_words) + \".\\n\"\n",
        "    prompt += \"\\n\".join(f\"- {c}\" for c in caps)\n",
        "\n",
        "    enc = flan_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    gen_kwargs = dict(\n",
        "        **enc,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        min_length=20,\n",
        "        num_beams=4,\n",
        "        length_penalty=1.15,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=1.1,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    # (HF supports force_words_ids for T5; usually not needed when we “encourage” in text)\n",
        "    ids = flan_model.generate(**gen_kwargs)\n",
        "    return flan_tokenizer.decode(ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_with_bart(frame_captions, encourage_words=None, max_length=80):\n",
        "    caps = curate_captions(frame_captions, max_caps=80)\n",
        "    if not caps: return \"\"\n",
        "    text = \"\\n\".join(caps)\n",
        "    enc = bart_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        input_ids=enc[\"input_ids\"],\n",
        "        attention_mask=enc.get(\"attention_mask\", None),\n",
        "        max_length=max_length,\n",
        "        min_length=20,\n",
        "        num_beams=4,\n",
        "        length_penalty=1.1,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=1.1,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    # Only force words that already appear in frame captions to avoid hallucinations\n",
        "    if encourage_words:\n",
        "        fw_ids = build_force_ids(bart_tokenizer, encourage_words)\n",
        "        if fw_ids:\n",
        "            gen_kwargs[\"force_words_ids\"] = fw_ids\n",
        "\n",
        "    out_ids = bart_model.generate(**gen_kwargs)\n",
        "    return bart_tokenizer.decode(out_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# ================================\n",
        "# 6) Frame extraction (4 fps)\n",
        "# ================================\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vid = cv2.VideoCapture(video_path)\n",
        "    if not vid.isOpened():\n",
        "        print(f\"  ! Could not open video: {video_path}\")\n",
        "        return 0\n",
        "    fps = vid.get(cv2.CAP_PROP_FPS) or 0\n",
        "    interval = int(max(1, round((fps if fps > 0 else 4)/4)))  # ~ every 0.25s\n",
        "    ok, frame = vid.read()\n",
        "    count, saved = 0, 0\n",
        "    while ok:\n",
        "        if count % interval == 0:\n",
        "            cv2.imwrite(os.path.join(output_folder, f\"frame_{saved:03d}.jpg\"), frame)\n",
        "            saved += 1\n",
        "        ok, frame = vid.read()\n",
        "        count += 1\n",
        "    vid.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7) Metadata loading & parsing\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "\n",
        "def parse_metadata(df):\n",
        "    meta = {}\n",
        "    for _, row in df.iterrows():\n",
        "        vid = row.get(\"video_id\")\n",
        "        animals_raw = row.get(\"list_animal\", [])\n",
        "        actions_raw = row.get(\"list_animal_action\", \"\")\n",
        "\n",
        "        # animals\n",
        "        if isinstance(animals_raw, str):\n",
        "            try:\n",
        "                animals = ast.literal_eval(animals_raw)\n",
        "                if not isinstance(animals, list):\n",
        "                    animals = [str(animals)]\n",
        "            except Exception:\n",
        "                animals = [a.strip() for a in animals_raw.split(\",\") if a.strip()]\n",
        "        elif isinstance(animals_raw, list):\n",
        "            animals = animals_raw\n",
        "        else:\n",
        "            animals = []\n",
        "\n",
        "        # actions\n",
        "        actions = []\n",
        "        if isinstance(actions_raw, str) and actions_raw.strip():\n",
        "            try:\n",
        "                parsed = ast.literal_eval(actions_raw)\n",
        "                if isinstance(parsed, list):\n",
        "                    for it in parsed:\n",
        "                        if isinstance(it, (list, tuple)) and len(it) >= 2:\n",
        "                            actions.append(str(it[1]))\n",
        "                        elif isinstance(it, str):\n",
        "                            actions.append(it)\n",
        "            except Exception:\n",
        "                actions = [a.strip() for a in actions_raw.split(\",\") if a.strip()]\n",
        "\n",
        "        meta[vid] = {\"animals\": animals, \"actions\": actions}\n",
        "    return meta\n",
        "\n",
        "metadata_dict = parse_metadata(meta_df)\n",
        "\n",
        "# ================================\n",
        "# 8) Robust semantic scoring\n",
        "# ================================\n",
        "ANIMAL_SYNONYMS = {\n",
        "    \"hippo\":\"hippopotamus\", \"gator\":\"alligator\", \"croc\":\"crocodile\",\n",
        "    \"lioness\":\"lion\", \"cheetahs\":\"cheetah\", \"elephants\":\"elephant\"\n",
        "}\n",
        "\n",
        "def normalize_tokens(text):\n",
        "    words = re.findall(r\"[a-zA-Z]+\", (text or \"\").lower())\n",
        "    lemmas = set()\n",
        "    for w in words:\n",
        "        base_n = lemmatizer.lemmatize(w, pos='n')\n",
        "        base_v = lemmatizer.lemmatize(w, pos='v')\n",
        "        lemmas.update([w, base_n, base_v, ANIMAL_SYNONYMS.get(w, w)])\n",
        "    return lemmas\n",
        "\n",
        "def lemma_phrase_match(caption, phrase):\n",
        "    cap_lemmas = normalize_tokens(caption)\n",
        "    toks = [t for t in re.findall(r\"[a-zA-Z]+\", (phrase or \"\").lower()) if t]\n",
        "    if not toks: return False\n",
        "    kw_lemmas = set()\n",
        "    for t in toks:\n",
        "        kw_lemmas.update([\n",
        "            t,\n",
        "            lemmatizer.lemmatize(t, pos='n'),\n",
        "            lemmatizer.lemmatize(t, pos='v'),\n",
        "            ANIMAL_SYNONYMS.get(t, t)\n",
        "        ])\n",
        "    return kw_lemmas.issubset(cap_lemmas)\n",
        "\n",
        "def semantic_scores(caption, animals, actions):\n",
        "    raw_animals, raw_actions = animals or [], actions or []\n",
        "\n",
        "    # Normalize metadata to caption-friendly forms first\n",
        "    animals_n, actions_n = normalize_meta_keywords(raw_animals, raw_actions)\n",
        "    keywords = [*animals_n, *actions_n]\n",
        "    keywords = [k for k in keywords if isinstance(k, str) and k.strip()]\n",
        "    if not keywords:\n",
        "        return 0.0, [], [], 0.0, [], [], animals_n, actions_n\n",
        "\n",
        "    cap_l = (caption or \"\").lower()\n",
        "    strict_matched, strict_missing = [], []\n",
        "    for kw in keywords:\n",
        "        if kw.lower() in cap_l:\n",
        "            strict_matched.append(kw)\n",
        "        else:\n",
        "            strict_missing.append(kw)\n",
        "    strict_score = len(strict_matched)/len(keywords) if keywords else 0.0\n",
        "\n",
        "    lemma_matched, lemma_missing = [], []\n",
        "    for kw in keywords:\n",
        "        if lemma_phrase_match(caption, kw):\n",
        "            lemma_matched.append(kw)\n",
        "        else:\n",
        "            lemma_missing.append(kw)\n",
        "    lemma_score = len(lemma_matched)/len(keywords) if keywords else 0.0\n",
        "\n",
        "    return strict_score, strict_matched, strict_missing, lemma_score, lemma_matched, lemma_missing, animals_n, actions_n\n",
        "\n",
        "# ================================\n",
        "# 9) Main loop\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:1000]\n",
        "\n",
        "SUMMARIZER = \"flan\"         # \"flan\" or \"bart\"\n",
        "USE_KEYWORD_STEERING = True # softly encourage only safe keywords\n",
        "\n",
        "results = []\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_dir = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\n Processing {video_id}\")\n",
        "\n",
        "    # Step 1: Extract frames\n",
        "    n_frames = extract_frames_4fps(video_path, frame_dir)\n",
        "    if n_frames == 0:\n",
        "        print(\"  ! No frames extracted; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Step 2: Caption each frame\n",
        "    frame_captions = []\n",
        "    frame_names = sorted([fn for fn in os.listdir(frame_dir) if fn.endswith(\".jpg\")])\n",
        "    for idx, fname in enumerate(frame_names, start=1):\n",
        "        path = os.path.join(frame_dir, fname)\n",
        "        caption = generate_caption(path)\n",
        "        frame_captions.append(caption)\n",
        "        if idx % 20 == 0:\n",
        "            print(f\"  - Captioned {idx}/{len(frame_names)} frames\")\n",
        "\n",
        "    # Step 3: Summarize\n",
        "    meta = metadata_dict.get(video_id, {\"animals\": [], \"actions\": []})\n",
        "    animals_raw, actions_raw = meta.get(\"animals\", []), meta.get(\"actions\", [])\n",
        "    # normalize metadata to caption-friendly forms\n",
        "    animals_n, actions_n = normalize_meta_keywords(animals_raw, actions_raw)\n",
        "    meta_keywords_norm = [*animals_n, *actions_n]\n",
        "\n",
        "    encourage = pick_keywords_to_encourage(frame_captions, meta_keywords_norm) if USE_KEYWORD_STEERING else None\n",
        "\n",
        "    if SUMMARIZER.lower() == \"flan\":\n",
        "        final_caption = summarize_with_flan(frame_captions, encourage_words=encourage, max_new_tokens=80)\n",
        "    else:\n",
        "        final_caption = summarize_with_bart(frame_captions, encourage_words=encourage, max_length=80)\n",
        "\n",
        "    # Step 4: Scoring (strict + lemma-aware)\n",
        "    s_score, s_match, s_miss, l_score, l_match, l_miss, animals_n, actions_n = \\\n",
        "        semantic_scores(final_caption, animals_raw, actions_raw)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions_curated\": curate_captions(frame_captions, max_caps=80),\n",
        "        \"keywords_normalized\": [*animals_n, *actions_n],\n",
        "        \"strict_matched\": s_match,\n",
        "        \"strict_missing\": s_miss,\n",
        "        \"strict_semantic_percent\": f\"{s_score*100:.1f}%\",\n",
        "        \"lemma_matched\": l_match,\n",
        "        \"lemma_missing\": l_miss,\n",
        "        \"lemma_semantic_percent\": f\"{l_score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 10) Save results\n",
        "# ================================\n",
        "out_path = \"/content/blip2_summary_results.csv\"\n",
        "pd.DataFrame(results).to_csv(out_path, index=False)\n",
        "\n",
        "print(\"\\n Done. Results saved to:\")\n",
        "print(f\" {out_path}\")\n"
      ],
      "metadata": {
        "id": "y7y6WKY0e2fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sIDNNjoAqndz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pUV6mgIfqnVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First 5 videos + FLAN-T5 + Robust semantic score"
      ],
      "metadata": {
        "id": "NtmjsHLwPCot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Process ONLY the first 5 videos with robust scoring\n",
        "# ================================\n",
        "!pip install -q git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas nltk\n",
        "\n",
        "import os, re, cv2, ast, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# --- Lemmatizer for robust scoring\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ---------- BLIP2 (frame captioning)\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        ")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device=blip_model.device, dtype=blip_model.dtype) for k, v in inputs.items()}\n",
        "    out = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    return blip_processor.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "# ---------- Summarizers\n",
        "flan_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "flan_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
        "\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def curate_captions(caps, max_caps=80, min_chars=8):\n",
        "    caps = [c.strip() for c in caps if isinstance(c, str) and len(c.strip()) >= min_chars]\n",
        "    seen, uniq = set(), []\n",
        "    for c in caps:\n",
        "        if c not in seen:\n",
        "            seen.add(c); uniq.append(c)\n",
        "    if len(uniq) <= max_caps:\n",
        "        return uniq\n",
        "    idxs = [round(i) for i in [j*(len(uniq)-1)/(max_caps-1) for j in range(max_caps)]]\n",
        "    return [uniq[i] for i in idxs]\n",
        "\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vid = cv2.VideoCapture(video_path)\n",
        "    if not vid.isOpened():\n",
        "        print(f\"  ! Could not open {video_path}\")\n",
        "        return 0\n",
        "    fps = vid.get(cv2.CAP_PROP_FPS) or 0\n",
        "    interval = int(max(1, round((fps if fps > 0 else 4)/4)))\n",
        "    ok, frame = vid.read()\n",
        "    count, saved = 0, 0\n",
        "    while ok:\n",
        "        if count % interval == 0:\n",
        "            cv2.imwrite(os.path.join(output_folder, f\"frame_{saved:03d}.jpg\"), frame)\n",
        "            saved += 1\n",
        "        ok, frame = vid.read()\n",
        "        count += 1\n",
        "    vid.release()\n",
        "    return saved\n",
        "\n",
        "# ---------- Metadata loading & normalization\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "\n",
        "def parse_metadata(df):\n",
        "    meta = {}\n",
        "    for _, row in df.iterrows():\n",
        "        vid = row.get(\"video_id\")\n",
        "        animals_raw = row.get(\"list_animal\", [])\n",
        "        actions_raw = row.get(\"list_animal_action\", \"\")\n",
        "\n",
        "        # animals\n",
        "        if isinstance(animals_raw, str):\n",
        "            try:\n",
        "                animals = ast.literal_eval(animals_raw)\n",
        "                if not isinstance(animals, list): animals = [str(animals)]\n",
        "            except Exception:\n",
        "                animals = [a.strip() for a in animals_raw.split(\",\") if a.strip()]\n",
        "        elif isinstance(animals_raw, list):\n",
        "            animals = animals_raw\n",
        "        else:\n",
        "            animals = []\n",
        "\n",
        "        # actions\n",
        "        actions = []\n",
        "        if isinstance(actions_raw, str) and actions_raw.strip():\n",
        "            try:\n",
        "                parsed = ast.literal_eval(actions_raw)\n",
        "                if isinstance(parsed, list):\n",
        "                    for it in parsed:\n",
        "                        if isinstance(it, (list, tuple)) and len(it) >= 2: actions.append(str(it[1]))\n",
        "                        elif isinstance(it, str): actions.append(it)\n",
        "            except Exception:\n",
        "                actions = [a.strip() for a in actions_raw.split(\",\") if a.strip()]\n",
        "\n",
        "        meta[vid] = {\"animals\": animals, \"actions\": actions}\n",
        "    return meta\n",
        "\n",
        "metadata_dict = parse_metadata(meta_df)\n",
        "\n",
        "# Map metadata -> caption-friendly forms\n",
        "SPECIES_MAP = {\n",
        "    \"eurasian wren bird\": \"wren\",\n",
        "    \"ardea alba egret\": \"egret\",\n",
        "    \"grey heron\": \"heron\",\n",
        "    \"black-winged stilt\": \"stilt\",\n",
        "    \"yellowhammer\": \"bird\",\n",
        "    \"tit bird\": \"tit\",\n",
        "    \"mallard duck\": \"duck\",\n",
        "    \"common crane\": \"crane\",\n",
        "    \"black mamba\": \"snake\",\n",
        "    \"mongoose\": \"mongoose\",\n",
        "}\n",
        "ACTION_MAP = {\n",
        "    \"keeping still\": \"standing still\",\n",
        "    \"attending\": \"looking\",\n",
        "    \"sensing\": \"looking\",\n",
        "    \"chirping\": \"singing\",\n",
        "    \"singing nightingale\": \"nightingale singing\",\n",
        "    \"flapping\": \"flapping\",\n",
        "    \"landing\": \"landing\",\n",
        "    \"preening\": \"preening\",\n",
        "    \"walking\": \"walking\",\n",
        "    \"moving\": \"moving\",\n",
        "    \"eating\": \"eating\",\n",
        "    \"jumping\": \"jumping\",\n",
        "    \"attacking\": \"attacking\",\n",
        "    \"flying\": \"flying\",\n",
        "    \"shaking head\": \"shaking head\",\n",
        "}\n",
        "\n",
        "def normalize_meta_keywords(animals, actions):\n",
        "    def norm_list(lst, mp):\n",
        "        out = []\n",
        "        for x in (lst or []):\n",
        "            x_l = str(x).lower()\n",
        "            out.append(mp.get(x_l, x_l))\n",
        "        return out\n",
        "    return norm_list(animals, SPECIES_MAP), norm_list(actions, ACTION_MAP)\n",
        "\n",
        "# ---------- Summarization\n",
        "def encourage_words(frame_captions, meta_keywords, max_animals=5, max_actions=3):\n",
        "    text = \" \".join(frame_captions).lower()\n",
        "    animals, actions = [], []\n",
        "    for kw in meta_keywords:\n",
        "        kw_l = kw.lower()\n",
        "        if kw_l in text:\n",
        "            if any(kw_l.endswith(s) for s in (\"ing\",\"ed\")) or \" \" in kw_l: actions.append(kw_l)\n",
        "            else: animals.append(kw_l)\n",
        "    return animals[:max_animals] + actions[:max_actions]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_with_flan(frame_captions, encourage=None, max_new_tokens=80):\n",
        "    caps = curate_captions(frame_captions, max_caps=80)\n",
        "    if not caps: return \"\"\n",
        "    prompt = \"Summarize the wildlife video in one detailed, factual sentence mentioning all animals and their actions.\\n\"\n",
        "    if encourage: prompt += \"Ensure to mention: \" + \", \".join(encourage) + \".\\n\"\n",
        "    prompt += \"\\n\".join(f\"- {c}\" for c in caps)\n",
        "    enc = flan_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    ids = flan_model.generate(\n",
        "        **enc, max_new_tokens=max_new_tokens, min_length=20,\n",
        "        num_beams=4, length_penalty=1.15, no_repeat_ngram_size=3,\n",
        "        repetition_penalty=1.1, early_stopping=True\n",
        "    )\n",
        "    return flan_tokenizer.decode(ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# ---------- Robust scoring (hypernyms + action inference + partial match)\n",
        "ANIMAL_BACKOFF = {\n",
        "    \"nightingale\": [\"nightingale\", \"bird\", \"songbird\"],\n",
        "    \"wren\": [\"wren\", \"bird\"],\n",
        "    \"egret\": [\"egret\", \"heron\", \"bird\"],\n",
        "    \"heron\": [\"heron\", \"bird\"],\n",
        "    \"stilt\": [\"stilt\", \"wader\", \"shorebird\", \"bird\"],\n",
        "    \"crane\": [\"crane\", \"bird\"],\n",
        "    \"duck\": [\"duck\", \"waterfowl\", \"bird\"],\n",
        "    \"tit\": [\"tit\", \"bird\"],\n",
        "    \"snake\": [\"snake\", \"serpent\", \"reptile\"],\n",
        "    \"mongoose\": [\"mongoose\", \"mammal\", \"animal\"],\n",
        "    \"bird\": [\"bird\"],\n",
        "    \"animal\": [\"animal\"],\n",
        "}\n",
        "\n",
        "def infer_actions_from_caption(text):\n",
        "    t = (text or \"\").lower()\n",
        "    inferred = set()\n",
        "    if any(p in t for p in [\"singing\",\"chirping\",\"trilling\",\"calling\",\"vocalizing\",\"song\"]):\n",
        "        inferred.update([\"singing\",\"chirping\",\"calling\"])\n",
        "    if any(p in t for p in [\"beak open\",\"mouth open\",\"open beak\",\"open mouth\"]):\n",
        "        inferred.update([\"singing\",\"calling\",\"chirping\"])\n",
        "    if any(p in t for p in [\"keeping still\",\"standing still\",\"motionless\",\"perched\",\"sitting\",\"resting\"]):\n",
        "        inferred.update([\"keeping still\",\"standing still\"])\n",
        "    if \"walking\" in t: inferred.add(\"walking\")\n",
        "    if \"running\" in t: inferred.add(\"running\")\n",
        "    if any(p in t for p in [\"flying\",\"in flight\",\"takes off\",\"taking off\",\"soars\",\"soaring\"]): inferred.add(\"flying\")\n",
        "    if any(p in t for p in [\"landing\",\"lands\",\"touches down\"]): inferred.add(\"landing\")\n",
        "    if any(p in t for p in [\"flapping\",\"flaps\"]): inferred.add(\"flapping\")\n",
        "    if any(p in t for p in [\"preening\",\"grooming feathers\"]): inferred.add(\"preening\")\n",
        "    if any(p in t for p in [\"eating\",\"feeding\",\"chewing\",\"grazing\",\"pecking\"]): inferred.add(\"eating\")\n",
        "    if any(p in t for p in [\"shaking head\",\"head shake\"]): inferred.add(\"shaking head\")\n",
        "    if any(p in t for p in [\"looking\",\"watching\",\"peering\",\"gazing\",\"staring\"]): inferred.add(\"looking\")\n",
        "    if any(p in t for p in [\"attacks\",\"attack\",\"attacking\",\"strikes\",\"strike\"]): inferred.add(\"attacking\")\n",
        "    if any(p in t for p in [\"jumping\",\"leaping\",\"hopping\"]): inferred.add(\"jumping\")\n",
        "    if \"moving\" in t: inferred.add(\"moving\")\n",
        "    return inferred\n",
        "\n",
        "def normalize_lemmas(text):\n",
        "    words = re.findall(r\"[a-zA-Z]+\", (text or \"\").lower())\n",
        "    return {lemmatizer.lemmatize(w, pos='n') for w in words} | {lemmatizer.lemmatize(w, pos='v') for w in words}\n",
        "\n",
        "def partial_phrase_match(caption, phrase, threshold=0.6):\n",
        "    cap = normalize_lemmas(caption)\n",
        "    toks = [t for t in re.findall(r\"[a-zA-Z]+\", (phrase or \"\").lower()) if t]\n",
        "    if not toks: return False\n",
        "    kw = {lemmatizer.lemmatize(t, pos='n') for t in toks} | {lemmatizer.lemmatize(t, pos='v') for t in toks}\n",
        "    overlap = sum(1 for t in kw if t in cap)\n",
        "    return (overlap / len(kw)) >= threshold\n",
        "\n",
        "def animal_match_with_backoff(caption, animal_kw):\n",
        "    cap = (caption or \"\").lower()\n",
        "    key = animal_kw.lower()\n",
        "    cand = ANIMAL_BACKOFF.get(key, [key])\n",
        "    return any(c in cap for c in cand) or partial_phrase_match(caption, key, threshold=0.5)\n",
        "\n",
        "def score_semantics(caption, animals_norm, actions_norm):\n",
        "    # Return NA if no keywords\n",
        "    all_kws = [*(animals_norm or []), *(actions_norm or [])]\n",
        "    if not all_kws:\n",
        "        return None, [], [], None, [], []\n",
        "    cap_l = (caption or \"\").lower()\n",
        "\n",
        "    # Strict (legacy)\n",
        "    s_match, s_miss = [], []\n",
        "    for kw in all_kws:\n",
        "        (s_match if kw.lower() in cap_l else s_miss).append(kw)\n",
        "    s_score = len(s_match)/len(all_kws)\n",
        "\n",
        "    # Robust\n",
        "    inferred = infer_actions_from_caption(caption)\n",
        "    r_match, r_miss = [], []\n",
        "    for kw in all_kws:\n",
        "        k = kw.lower()\n",
        "        if k in (animals_norm or []):\n",
        "            ok = animal_match_with_backoff(caption, k)\n",
        "        else:\n",
        "            ok = (k in inferred) or partial_phrase_match(caption, k, threshold=0.6)\n",
        "        (r_match if ok else r_miss).append(kw)\n",
        "    r_score = len(r_match)/len(all_kws)\n",
        "    return s_score, s_match, s_miss, r_score, r_match, r_miss\n",
        "\n",
        "# ---------- Main: only first 5 videos\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:5]\n",
        "\n",
        "SUMMARIZER = \"flan\"      # \"flan\" (recommended) or \"bart\"\n",
        "USE_STEERING = True\n",
        "\n",
        "rows = []\n",
        "for vf in video_files:\n",
        "    vid = os.path.splitext(vf)[0]\n",
        "    vpath = os.path.join(video_dir, vf)\n",
        "    fdir = f\"/content/frames/{vid}\"\n",
        "\n",
        "    print(f\"\\n Processing {vid}\")\n",
        "    n = extract_frames_4fps(vpath, fdir)\n",
        "    if n == 0:\n",
        "        print(\"  ! Skipping (no frames).\")\n",
        "        continue\n",
        "\n",
        "    # Captions\n",
        "    caps = []\n",
        "    frames = sorted([fn for fn in os.listdir(fdir) if fn.endswith(\".jpg\")])\n",
        "    for i, fn in enumerate(frames, 1):\n",
        "        caps.append(generate_caption(os.path.join(fdir, fn)))\n",
        "        if i % 20 == 0:\n",
        "            print(f\"  - Captioned {i}/{len(frames)} frames\")\n",
        "    caps_cur = curate_captions(caps, max_caps=80)\n",
        "\n",
        "    # Metadata\n",
        "    meta = metadata_dict.get(vid, {\"animals\": [], \"actions\": []})\n",
        "    animals_raw, actions_raw = meta.get(\"animals\", []), meta.get(\"actions\", [])\n",
        "    animals_n, actions_n = normalize_meta_keywords(animals_raw, actions_raw)\n",
        "    kws_norm = [*animals_n, *actions_n]\n",
        "\n",
        "    # Summarize\n",
        "    encouragement = encourage_words(caps_cur, kws_norm) if USE_STEERING else None\n",
        "    if SUMMARIZER == \"flan\":\n",
        "        final_caption = summarize_with_flan(caps_cur, encourage=encouragement, max_new_tokens=80)\n",
        "    else:\n",
        "        # Optional: swap in BART if you want to compare\n",
        "        enc = bart_tokenizer(\"\\n\".join(caps_cur), return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "        out_ids = bart_model.generate(\n",
        "            input_ids=enc[\"input_ids\"], attention_mask=enc.get(\"attention_mask\", None),\n",
        "            max_length=80, min_length=20, num_beams=4, length_penalty=1.1,\n",
        "            no_repeat_ngram_size=3, repetition_penalty=1.1, early_stopping=True\n",
        "        )\n",
        "        final_caption = bart_tokenizer.decode(out_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Scores\n",
        "    s_score, s_match, s_miss, r_score, r_match, r_miss = score_semantics(final_caption, animals_n, actions_n)\n",
        "\n",
        "    rows.append({\n",
        "        \"video_id\": vid,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions_curated\": caps_cur,\n",
        "        \"keywords_normalized\": kws_norm,\n",
        "        \"strict_matched\": s_match, \"strict_missing\": s_miss,\n",
        "        \"robust_matched\": r_match, \"robust_missing\": r_miss,\n",
        "        \"strict_semantic_percent\": \"NA\" if s_score is None else f\"{s_score*100:.1f}%\",\n",
        "        \"robust_semantic_percent\": \"NA\" if r_score is None else f\"{r_score*100:.1f}%\"\n",
        "    })\n",
        "\n",
        "# Save & print a tiny summary\n",
        "out_path = \"/content/blip2_summary_results_first5.csv\"\n",
        "pd.DataFrame(rows).to_csv(out_path, index=False)\n",
        "print(\"\\nSaved:\", out_path)\n",
        "\n",
        "for r in rows:\n",
        "    print(f\"\\n[{r['video_id']}]\")\n",
        "    print(\"Final:\", r[\"final_caption\"])\n",
        "    print(\"Strict:\", r[\"strict_semantic_percent\"], \" | Robust:\", r[\"robust_semantic_percent\"])\n",
        "    if r[\"robust_missing\"]:\n",
        "        print(\"Robust missing (few):\", r[\"robust_missing\"][:5])"
      ],
      "metadata": {
        "id": "iJ_h-DAsqnF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_-AbqScwqM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XbE4U5XBwqJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_gCeOmlwqGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "First 1000(FLAN-T5)"
      ],
      "metadata": {
        "id": "kXR0PcnfOyRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1) Install\n",
        "# ================================\n",
        "# !pip install -q git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas nltk\n",
        "\n",
        "# ================================\n",
        "# 2) Imports\n",
        "# ================================\n",
        "import os, re, cv2, ast, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration,\n",
        "    T5Tokenizer, T5ForConditionalGeneration\n",
        ")\n",
        "\n",
        "# Lemmatization for robust scoring\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# ================================\n",
        "# 3) Device\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "\n",
        "# ================================\n",
        "# 4) BLIP2 (frame captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        ")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device=blip_model.device, dtype=blip_model.dtype) for k, v in inputs.items()}\n",
        "    out = blip_model.generate(**inputs, max_new_tokens=50)\n",
        "    return blip_processor.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "# ================================\n",
        "# 5) Summarizers: FLAN (default) & BART\n",
        "# ================================\n",
        "flan_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "flan_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
        "\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def curate_captions(caps, max_caps=80, min_chars=8):\n",
        "    caps = [c.strip() for c in caps if isinstance(c, str) and len(c.strip()) >= min_chars]\n",
        "    seen, uniq = set(), []\n",
        "    for c in caps:\n",
        "        if c not in seen:\n",
        "            seen.add(c); uniq.append(c)\n",
        "    if len(uniq) <= max_caps: return uniq\n",
        "    idxs = [round(i) for i in [j*(len(uniq)-1)/(max_caps-1) for j in range(max_caps)]]\n",
        "    return [uniq[i] for i in idxs]\n",
        "\n",
        "# --- Normalize metadata to caption-friendly forms\n",
        "SPECIES_MAP = {\n",
        "    \"eurasian wren bird\": \"wren\",\n",
        "    \"ardea alba egret\": \"egret\",\n",
        "    \"grey heron\": \"heron\",\n",
        "    \"black-winged stilt\": \"stilt\",\n",
        "    \"yellowhammer\": \"bird\",\n",
        "    \"tit bird\": \"tit\",\n",
        "    \"mallard duck\": \"duck\",\n",
        "    \"common crane\": \"crane\",\n",
        "    \"black mamba\": \"snake\",\n",
        "    \"mongoose\": \"mongoose\",\n",
        "}\n",
        "ACTION_MAP = {\n",
        "    \"keeping still\": \"standing still\",\n",
        "    \"attending\": \"looking\",\n",
        "    \"sensing\": \"looking\",\n",
        "    \"chirping\": \"singing\",\n",
        "    \"flapping\": \"flapping\",\n",
        "    \"landing\": \"landing\",\n",
        "    \"preening\": \"preening\",\n",
        "    \"walking\": \"walking\",\n",
        "    \"moving\": \"moving\",\n",
        "    \"eating\": \"eating\",\n",
        "    \"jumping\": \"jumping\",\n",
        "    \"attacking\": \"attacking\",\n",
        "    \"flying\": \"flying\",\n",
        "    \"shaking head\": \"shaking head\",\n",
        "    \"singing nightingale\": \"nightingale singing\",\n",
        "}\n",
        "\n",
        "def normalize_meta_keywords(animals, actions):\n",
        "    def norm_list(lst, mp):\n",
        "        out = []\n",
        "        for x in (lst or []):\n",
        "            x_l = str(x).lower()\n",
        "            out.append(mp.get(x_l, x_l))\n",
        "        return out\n",
        "    return norm_list(animals, SPECIES_MAP), norm_list(actions, ACTION_MAP)\n",
        "\n",
        "# ---- Steering: now with fallback to metadata if nothing found in captions\n",
        "def pick_keywords_to_encourage(frame_captions, meta_keywords, max_animals=8, max_actions=6):\n",
        "    text = \" \".join(frame_captions).lower()\n",
        "    animals, actions = [], []\n",
        "    for kw in meta_keywords:\n",
        "        k = kw.lower()\n",
        "        if k in text:\n",
        "            if any(k.endswith(s) for s in (\"ing\",\"ed\")) or \" \" in k: actions.append(k)\n",
        "            else: animals.append(k)\n",
        "    # Fallback: if nothing surfaced from captions, nudge a small slice of trusted metadata\n",
        "    if not animals and not actions:\n",
        "        animals = [k for k in meta_keywords if \" \" not in k][:max_animals]\n",
        "        actions = [k for k in meta_keywords if (\" \" in k) or k.endswith((\"ing\",\"ed\"))][:max_actions]\n",
        "    return animals[:max_animals] + actions[:max_actions]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_with_flan(frame_captions, encourage_words=None, max_new_tokens=80):\n",
        "    caps = curate_captions(frame_captions, max_caps=80)\n",
        "    if not caps: return \"\"\n",
        "    prompt = \"Summarize the wildlife video in one detailed, factual sentence mentioning all animals and their actions.\\n\"\n",
        "    if encourage_words: prompt += \"Ensure to mention: \" + \", \".join(encourage_words) + \".\\n\"\n",
        "    prompt += \"\\n\".join(f\"- {c}\" for c in caps)\n",
        "    enc = flan_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    ids = flan_model.generate(\n",
        "        **enc, max_new_tokens=max_new_tokens, min_length=20, num_beams=4,\n",
        "        length_penalty=1.15, no_repeat_ngram_size=3, repetition_penalty=1.1,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return flan_tokenizer.decode(ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_with_bart(frame_captions, encourage_words=None, max_length=80):\n",
        "    caps = curate_captions(frame_captions, max_caps=80)\n",
        "    if not caps: return \"\"\n",
        "    text = \"\\n\".join(caps)\n",
        "    enc = bart_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    gen_kwargs = dict(\n",
        "        input_ids=enc[\"input_ids\"], attention_mask=enc.get(\"attention_mask\", None),\n",
        "        max_length=max_length, min_length=20, num_beams=4,\n",
        "        length_penalty=1.1, no_repeat_ngram_size=3, repetition_penalty=1.1,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    # Optional: force a few trusted keywords\n",
        "    if encourage_words:\n",
        "        fw = []\n",
        "        for w in encourage_words[:8]:\n",
        "            ids = bart_tokenizer(w, add_special_tokens=False).input_ids\n",
        "            if ids: fw.append(ids)\n",
        "        if fw: gen_kwargs[\"force_words_ids\"] = fw\n",
        "    out = bart_model.generate(**gen_kwargs)\n",
        "    return bart_tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# ================================\n",
        "# 6) Frame extraction (4 fps)\n",
        "# ================================\n",
        "def extract_frames_4fps(video_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vid = cv2.VideoCapture(video_path)\n",
        "    if not vid.isOpened():\n",
        "        print(f\"  ! Could not open video: {video_path}\")\n",
        "        return 0\n",
        "    fps = vid.get(cv2.CAP_PROP_FPS) or 0\n",
        "    interval = int(max(1, round((fps if fps > 0 else 4)/4)))  # ~ every 0.25s\n",
        "    ok, frame = vid.read()\n",
        "    count, saved = 0, 0\n",
        "    while ok:\n",
        "        if count % interval == 0:\n",
        "            cv2.imwrite(os.path.join(output_folder, f\"frame_{saved:03d}.jpg\"), frame)\n",
        "            saved += 1\n",
        "        ok, frame = vid.read(); count += 1\n",
        "    vid.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7) Metadata loading & parsing\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "\n",
        "def parse_metadata(df):\n",
        "    meta = {}\n",
        "    for _, row in df.iterrows():\n",
        "        vid = row.get(\"video_id\")\n",
        "        animals_raw = row.get(\"list_animal\", [])\n",
        "        actions_raw = row.get(\"list_animal_action\", \"\")\n",
        "        # animals\n",
        "        if isinstance(animals_raw, str):\n",
        "            try:\n",
        "                animals = ast.literal_eval(animals_raw)\n",
        "                if not isinstance(animals, list): animals = [str(animals)]\n",
        "            except Exception:\n",
        "                animals = [a.strip() for a in animals_raw.split(\",\") if a.strip()]\n",
        "        elif isinstance(animals_raw, list):\n",
        "            animals = animals_raw\n",
        "        else:\n",
        "            animals = []\n",
        "        # actions\n",
        "        actions = []\n",
        "        if isinstance(actions_raw, str) and actions_raw.strip():\n",
        "            try:\n",
        "                parsed = ast.literal_eval(actions_raw)\n",
        "                if isinstance(parsed, list):\n",
        "                    for it in parsed:\n",
        "                        if isinstance(it, (list, tuple)) and len(it) >= 2: actions.append(str(it[1]))\n",
        "                        elif isinstance(it, str): actions.append(it)\n",
        "            except Exception:\n",
        "                actions = [a.strip() for a in actions_raw.split(\",\") if a.strip()]\n",
        "        meta[vid] = {\"animals\": animals, \"actions\": actions}\n",
        "    return meta\n",
        "\n",
        "metadata_dict = parse_metadata(meta_df)\n",
        "\n",
        "# ================================\n",
        "# 8) Robust semantic scoring (short & general)\n",
        "# ================================\n",
        "# Small hypernym/backoff to avoid brittle species matches\n",
        "ANIMAL_BACKOFF = {\n",
        "    \"nightingale\": [\"nightingale\", \"bird\", \"songbird\"],\n",
        "    \"wren\": [\"wren\", \"bird\"],\n",
        "    \"egret\": [\"egret\", \"heron\", \"bird\"],\n",
        "    \"heron\": [\"heron\", \"bird\"],\n",
        "    \"stilt\": [\"stilt\", \"wader\", \"shorebird\", \"bird\"],\n",
        "    \"crane\": [\"crane\", \"bird\"],\n",
        "    \"duck\": [\"duck\", \"waterfowl\", \"bird\"],\n",
        "    \"tit\": [\"tit\", \"bird\"],\n",
        "    \"snake\": [\"snake\", \"serpent\", \"reptile\"],\n",
        "    \"mongoose\": [\"mongoose\", \"mammal\", \"animal\"],\n",
        "    \"bird\": [\"bird\"],\n",
        "    \"animal\": [\"animal\"],\n",
        "}\n",
        "\n",
        "def normalize_lemmas(text):\n",
        "    words = re.findall(r\"[a-zA-Z]+\", (text or \"\").lower())\n",
        "    return {lemmatizer.lemmatize(w, pos='n') for w in words} | {lemmatizer.lemmatize(w, pos='v') for w in words}\n",
        "\n",
        "def partial_phrase_match(caption, phrase, thr=0.6):\n",
        "    cap = normalize_lemmas(caption)\n",
        "    toks = [t for t in re.findall(r\"[a-zA-Z]+\", (phrase or \"\").lower()) if t]\n",
        "    if not toks: return False\n",
        "    kw = {lemmatizer.lemmatize(t, pos='n') for t in toks} | {lemmatizer.lemmatize(t, pos='v') for t in toks}\n",
        "    return (sum(1 for t in kw if t in cap) / len(kw)) >= thr\n",
        "\n",
        "def animal_match_with_backoff(caption, animal_kw):\n",
        "    cap = (caption or \"\").lower()\n",
        "    key = animal_kw.lower()\n",
        "    cand = ANIMAL_BACKOFF.get(key, [key])\n",
        "    return any(c in cap for c in cand) or partial_phrase_match(caption, key, thr=0.5)\n",
        "\n",
        "def _has_any(t, phrases): return any(p in t for p in phrases)\n",
        "\n",
        "def infer_actions_from_caption(text):\n",
        "    t = (text or \"\").lower()\n",
        "    inferred = set()\n",
        "    if _has_any(t, [\"singing\",\"chirping\",\"trilling\",\"calling\",\"vocalizing\",\"song\"]):\n",
        "        inferred.update([\"singing\",\"chirping\",\"calling\"])\n",
        "    if _has_any(t, [\"beak open\",\"open beak\",\"mouth open\",\"open mouth\"]):\n",
        "        inferred.update([\"singing\",\"calling\",\"chirping\"])\n",
        "    if _has_any(t, [\"keeping still\",\"standing still\",\"motionless\",\"perched\",\"sitting\",\"resting\",\"standing\"]):\n",
        "        inferred.update([\"keeping still\",\"standing still\",\"looking\"])\n",
        "    if \"walking\" in t: inferred.add(\"walking\")\n",
        "    if \"running\" in t: inferred.add(\"running\")\n",
        "    if _has_any(t, [\"flying\",\"in flight\",\"soars\",\"soaring\",\"taking off\",\"takes off\"]): inferred.add(\"flying\")\n",
        "    if _has_any(t, [\"landing\",\"lands\",\"touches down\"]): inferred.add(\"landing\")\n",
        "    if _has_any(t, [\"flapping\",\"flaps\"]): inferred.add(\"flapping\")\n",
        "    if _has_any(t, [\"eating\",\"feeding\",\"chewing\",\"grazing\",\"pecking\"]):\n",
        "        inferred.add(\"eating\")\n",
        "        if _has_any(t, [\"eating a\", \"eating the\", \"eating prey\", \"eating a snake\", \"eats a\"]):\n",
        "            inferred.update([\"biting\",\"attacking\"])\n",
        "    if _has_any(t, [\"biting\",\"bites\",\"bite\"]): inferred.update([\"biting\",\"attacking\"])\n",
        "    if _has_any(t, [\"dead \", \" dead\", \"carcass\", \"lifeless\"]): inferred.add(\"dying\")\n",
        "    if _has_any(t, [\"shaking head\",\"head shake\"]): inferred.add(\"shaking head\")\n",
        "    if _has_any(t, [\"looking\",\"watching\",\"peering\",\"gazing\",\"staring\"]): inferred.add(\"looking\")\n",
        "    if \"moving\" in t: inferred.add(\"moving\")\n",
        "    return inferred\n",
        "\n",
        "def robust_semantic_scores(caption, animals_norm, actions_norm):\n",
        "    \"\"\"\n",
        "    Returns both strict% (substring) and robust% (partial lemma + inference + hypernym).\n",
        "    \"\"\"\n",
        "    keywords = [*(animals_norm or []), *(actions_norm or [])]\n",
        "    keywords = [k for k in keywords if isinstance(k, str) and k.strip()]\n",
        "    if not keywords:\n",
        "        return \"NA\", [], [], \"NA\", [], []\n",
        "\n",
        "    cap_l = (caption or \"\").lower()\n",
        "\n",
        "    # strict (for continuity)\n",
        "    s_match, s_miss = [], []\n",
        "    for kw in keywords:\n",
        "        (s_match if kw.lower() in cap_l else s_miss).append(kw)\n",
        "    s_pct = f\"{(len(s_match)/len(keywords))*100:.1f}%\"\n",
        "\n",
        "    # robust\n",
        "    inferred = infer_actions_from_caption(caption)\n",
        "    r_match, r_miss = [], []\n",
        "    for kw in keywords:\n",
        "        k = kw.lower()\n",
        "        if k in (animals_norm or []):\n",
        "            ok = animal_match_with_backoff(caption, k)\n",
        "        else:\n",
        "            ok = (k in inferred) or partial_phrase_match(caption, k, thr=0.6)\n",
        "        (r_match if ok else r_miss).append(kw)\n",
        "    r_pct = f\"{(len(r_match)/len(keywords))*100:.1f}%\"\n",
        "\n",
        "    return s_pct, s_match, s_miss, r_pct, r_match, r_miss\n",
        "\n",
        "# ================================\n",
        "# 9) Main loop\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[:1000]\n",
        "\n",
        "SUMMARIZER = \"flan\"         # \"flan\" (recommended) or \"bart\"\n",
        "USE_KEYWORD_STEERING = True\n",
        "\n",
        "results = []\n",
        "for video_file in video_files:\n",
        "    video_id = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    frame_dir = f\"/content/frames/{video_id}\"\n",
        "\n",
        "    print(f\"\\n Processing {video_id}\")\n",
        "    n_frames = extract_frames_4fps(video_path, frame_dir)\n",
        "    if n_frames == 0:\n",
        "        print(\"  ! No frames extracted; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # captions\n",
        "    frame_names = sorted([fn for fn in os.listdir(frame_dir) if fn.endswith(\".jpg\")])\n",
        "    frame_captions = [generate_caption(os.path.join(frame_dir, fn)) for fn in frame_names]\n",
        "\n",
        "    # metadata (normalize to caption-friendly forms)\n",
        "    meta = metadata_dict.get(video_id, {\"animals\": [], \"actions\": []})\n",
        "    animals_raw, actions_raw = meta.get(\"animals\", []), meta.get(\"actions\", [])\n",
        "    animals_n, actions_n = normalize_meta_keywords(animals_raw, actions_raw)\n",
        "    meta_keywords_norm = [*animals_n, *actions_n]\n",
        "\n",
        "    encourage = pick_keywords_to_encourage(frame_captions, meta_keywords_norm) if USE_KEYWORD_STEERING else None\n",
        "\n",
        "    # summary\n",
        "    if SUMMARIZER.lower() == \"flan\":\n",
        "        final_caption = summarize_with_flan(frame_captions, encourage_words=encourage, max_new_tokens=80)\n",
        "    else:\n",
        "        final_caption = summarize_with_bart(frame_captions, encourage_words=encourage, max_length=80)\n",
        "\n",
        "    # scores\n",
        "    strict_pct, strict_matched, strict_missing, robust_pct, robust_matched, robust_missing = \\\n",
        "        robust_semantic_scores(final_caption, animals_n, actions_n)\n",
        "\n",
        "    results.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"final_caption\": final_caption,\n",
        "        \"frame_captions_curated\": curate_captions(frame_captions, max_caps=80),\n",
        "        \"keywords_normalized\": meta_keywords_norm,\n",
        "        \"strict_matched\": strict_matched,\n",
        "        \"strict_missing\": strict_missing,\n",
        "        \"strict_semantic_percent\": strict_pct,\n",
        "        \"robust_matched\": robust_matched,\n",
        "        \"robust_missing\": robust_missing,\n",
        "        \"robust_semantic_percent\": robust_pct\n",
        "    })\n",
        "\n",
        "# ================================\n",
        "# 10) Save\n",
        "# ================================\n",
        "out_path = \"/content/summary_results.csv\"\n",
        "pd.DataFrame(results).to_csv(out_path, index=False)\n",
        "print(\"\\n Done. Results saved to:\", out_path)\n"
      ],
      "metadata": {
        "id": "IFZo7ybNwqD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Point this to your output CSV (change if needed)\n",
        "PATH = \"/content/summary_results.csv\"\n",
        "\n",
        "df = pd.read_csv(PATH)\n",
        "\n",
        "def to_num(series):\n",
        "    # handles \"73.5%\", \"NA\", floats, etc.\n",
        "    s = series.astype(str).str.strip().str.replace(\"%\", \"\", regex=False)\n",
        "    return pd.to_numeric(s, errors=\"coerce\")\n",
        "\n",
        "stats = {}\n",
        "\n",
        "# Try robust first (recommended), then strict if present\n",
        "for col in [\"robust_semantic_percent\", \"strict_semantic_percent\"]:\n",
        "    if col in df.columns:\n",
        "        vals = to_num(df[col]).dropna()\n",
        "        stats[col] = {\n",
        "            \"count\": int(vals.count()),\n",
        "            \"mean\": float(vals.mean()) if not vals.empty else None,\n",
        "            \"median\": float(vals.median()) if not vals.empty else None,\n",
        "            \"std\": float(vals.std(ddof=1)) if len(vals) > 1 else 0.0,\n",
        "            \"min\": float(vals.min()) if not vals.empty else None,\n",
        "            \"max\": float(vals.max()) if not vals.empty else None,\n",
        "            \"zero_count\": int((vals == 0).sum()),\n",
        "        }\n",
        "\n",
        "for k, v in stats.items():\n",
        "    print(f\"\\n{k}:\")\n",
        "    for kk, vv in v.items():\n",
        "        if isinstance(vv, float):\n",
        "            print(f\"  {kk}: {vv:.2f}\")\n",
        "        else:\n",
        "            print(f\"  {kk}: {vv}\")\n",
        "\n",
        "# Save a small summary CSV next to the results\n",
        "summary_df = pd.DataFrame(stats).T\n",
        "summary_path = PATH.replace(\".csv\", \"_summary_stats.csv\")\n",
        "summary_df.to_csv(summary_path)\n",
        "print(\"\\nSaved summary to:\", summary_path)\n"
      ],
      "metadata": {
        "id": "22AKAgoBW687"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "flCNCZXVfyG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "my2KBJ9zfxp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random 1000(FLAN-T5)"
      ],
      "metadata": {
        "id": "ed9H6RiBf1Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1) Install\n",
        "# ================================\n",
        "!pip install -q git+https://github.com/salesforce/BLIP.git transformers timm accelerate opencv-python pandas nltk\n",
        "\n",
        "# ================================\n",
        "# 2) Imports\n",
        "# ================================\n",
        "import os, re, cv2, ast, math, random, gc, time, torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from typing import List\n",
        "from transformers import (\n",
        "    Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# Lemmatization for robust scoring\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# ================================\n",
        "# 3) Device + perf knobs\n",
        "# ================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Using device:\", device)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# ================================\n",
        "# 4) BLIP2 (batched frame captioning)\n",
        "# ================================\n",
        "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        ")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def batch_generate_captions(image_paths: List[str], batch_size: int = 4, max_new_tokens: int = 35) -> List[str]:\n",
        "    \"\"\"\n",
        "    Batched BLIP2 captioning for speed. Greedy decoding (do_sample=False).\n",
        "    \"\"\"\n",
        "    captions = []\n",
        "    use_amp = (device == \"cuda\")\n",
        "    for i in range(0, len(image_paths), batch_size):\n",
        "        batch_paths = image_paths[i:i+batch_size]\n",
        "        images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
        "\n",
        "        if use_amp:\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=blip_model.dtype):\n",
        "                inputs = blip_processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "                inputs = {k: v.to(device=blip_model.device, dtype=blip_model.dtype) for k,v in inputs.items()}\n",
        "                out = blip_model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "        else:\n",
        "            inputs = blip_processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "            inputs = {k: v.to(device=blip_model.device, dtype=blip_model.dtype) for k,v in inputs.items()}\n",
        "            out = blip_model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "\n",
        "        caps = blip_processor.batch_decode(out, skip_special_tokens=True)\n",
        "        captions.extend([c.strip() for c in caps])\n",
        "        for im in images: im.close()\n",
        "        del inputs, out\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        gc.collect()\n",
        "    return captions\n",
        "\n",
        "# ================================\n",
        "# 5) Summarizers (FLAN default, BART optional)\n",
        "# ================================\n",
        "flan_tok = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "flan = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
        "\n",
        "bart_tok = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def curate_captions(caps, max_caps=80, min_chars=8):\n",
        "    \"\"\"De-dup + uniformly sample to avoid 1024-token truncation.\"\"\"\n",
        "    caps = [c.strip() for c in caps if isinstance(c, str) and len(c.strip()) >= min_chars]\n",
        "    seen, uniq = set(), []\n",
        "    for c in caps:\n",
        "        if c not in seen:\n",
        "            seen.add(c); uniq.append(c)\n",
        "    if len(uniq) <= max_caps: return uniq\n",
        "    idxs = [round(i) for i in [j*(len(uniq)-1)/(max_caps-1) for j in range(max_caps)]]\n",
        "    return [uniq[i] for i in idxs]\n",
        "\n",
        "# Normalize metadata (dataset label -> caption-friendly)\n",
        "SPECIES_MAP = {\n",
        "    \"eurasian wren bird\":\"wren\",\"ardea alba egret\":\"egret\",\"grey heron\":\"heron\",\n",
        "    \"black-winged stilt\":\"stilt\",\"yellowhammer\":\"bird\",\"tit bird\":\"tit\",\n",
        "    \"mallard duck\":\"duck\",\"common crane\":\"crane\",\"black mamba\":\"snake\",\"mongoose\":\"mongoose\",\n",
        "}\n",
        "ACTION_MAP = {\n",
        "    \"keeping still\":\"standing still\",\"attending\":\"looking\",\"sensing\":\"looking\",\n",
        "    \"chirping\":\"singing\",\"flapping\":\"flapping\",\"landing\":\"landing\",\"preening\":\"preening\",\n",
        "    \"walking\":\"walking\",\"moving\":\"moving\",\"eating\":\"eating\",\"jumping\":\"jumping\",\n",
        "    \"attacking\":\"attacking\",\"flying\":\"flying\",\"shaking head\":\"shaking head\",\n",
        "    \"singing nightingale\":\"nightingale singing\"\n",
        "}\n",
        "\n",
        "def normalize_meta_keywords(animals, actions):\n",
        "    def norm(lst, mp):\n",
        "        out = []\n",
        "        for x in (lst or []):\n",
        "            x_l = str(x).lower()\n",
        "            out.append(mp.get(x_l, x_l))\n",
        "        return out\n",
        "    return norm(animals, SPECIES_MAP), norm(actions, ACTION_MAP)\n",
        "\n",
        "def pick_keywords_to_encourage(frame_captions, meta_keywords, max_animals=8, max_actions=6):\n",
        "    \"\"\"\n",
        "    Try to encourage words observed in captions; if none, fallback to a small slice of trusted metadata.\n",
        "    \"\"\"\n",
        "    text = \" \".join(frame_captions).lower()\n",
        "    animals, actions = [], []\n",
        "    for kw in meta_keywords:\n",
        "        k = kw.lower()\n",
        "        if k in text:\n",
        "            if any(k.endswith(s) for s in (\"ing\",\"ed\")) or \" \" in k: actions.append(k)\n",
        "            else: animals.append(k)\n",
        "    if not animals and not actions:\n",
        "        animals = [k for k in meta_keywords if \" \" not in k][:max_animals]\n",
        "        actions = [k for k in meta_keywords if (\" \" in k) or k.endswith((\"ing\",\"ed\"))][:max_actions]\n",
        "    return animals[:max_animals] + actions[:max_actions]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_flan(frame_captions, encourage=None, max_new_tokens=60):\n",
        "    caps = curate_captions(frame_captions, max_caps=80)\n",
        "    if not caps: return \"\"\n",
        "    prompt = \"Summarize the wildlife video in one detailed, factual sentence mentioning all animals and their actions.\\n\"\n",
        "    if encourage: prompt += \"Ensure to mention: \" + \", \".join(encourage) + \".\\n\"\n",
        "    prompt += \"\\n\".join(f\"- {c}\" for c in caps)\n",
        "    enc = flan_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    ids = flan.generate(\n",
        "        **enc, max_new_tokens=max_new_tokens, min_length=18,\n",
        "        num_beams=1, do_sample=False, no_repeat_ngram_size=3,\n",
        "        length_penalty=1.0, early_stopping=True\n",
        "    )\n",
        "    return flan_tok.decode(ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_bart(frame_captions, encourage=None, max_length=60):\n",
        "    caps = curate_captions(frame_captions, max_caps=80)\n",
        "    if not caps: return \"\"\n",
        "    text = \"\\n\".join(caps)\n",
        "    enc = bart_tok(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    gen_kwargs = dict(\n",
        "        input_ids=enc[\"input_ids\"], attention_mask=enc.get(\"attention_mask\", None),\n",
        "        max_length=max_length, min_length=18, num_beams=1, do_sample=False,\n",
        "        no_repeat_ngram_size=3, length_penalty=1.0, early_stopping=True\n",
        "    )\n",
        "    if encourage:\n",
        "        fw = []\n",
        "        for w in encourage[:8]:\n",
        "            ids = bart_tok(w, add_special_tokens=False).input_ids\n",
        "            if ids: fw.append(ids)\n",
        "        if fw: gen_kwargs[\"force_words_ids\"] = fw\n",
        "    out = bart.generate(**gen_kwargs)\n",
        "    return bart_tok.decode(out[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# ================================\n",
        "# 6) Frame extraction (cap frames for speed)\n",
        "# ================================\n",
        "def extract_frames_4fps(video_path, out_dir, max_frames=96):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    vid = cv2.VideoCapture(video_path)\n",
        "    if not vid.isOpened():\n",
        "        print(f\"  ! Could not open {video_path}\")\n",
        "        return 0\n",
        "    fps = vid.get(cv2.CAP_PROP_FPS) or 0\n",
        "    interval = int(max(1, round((fps if fps > 0 else 4)/4)))  # ~ every 0.25s\n",
        "    ok, frame = vid.read()\n",
        "    count, saved = 0, 0\n",
        "    while ok and saved < max_frames:\n",
        "        if count % interval == 0:\n",
        "            cv2.imwrite(os.path.join(out_dir, f\"frame_{saved:03d}.jpg\"), frame)\n",
        "            saved += 1\n",
        "        ok, frame = vid.read(); count += 1\n",
        "    vid.release()\n",
        "    return saved\n",
        "\n",
        "# ================================\n",
        "# 7) Metadata loading & parsing\n",
        "# ================================\n",
        "metadata_path = \"/content/drive/MyDrive/Animal_Kingdom/action_recognition/AR_metadata.xlsx\"\n",
        "meta_df = pd.read_excel(metadata_path)\n",
        "\n",
        "def parse_metadata(df):\n",
        "    meta = {}\n",
        "    for _, row in df.iterrows():\n",
        "        vid = row.get(\"video_id\")\n",
        "        animals_raw = row.get(\"list_animal\", [])\n",
        "        actions_raw = row.get(\"list_animal_action\", \"\")\n",
        "        # animals\n",
        "        if isinstance(animals_raw, str):\n",
        "            try:\n",
        "                animals = ast.literal_eval(animals_raw)\n",
        "                if not isinstance(animals, list): animals = [str(animals)]\n",
        "            except Exception:\n",
        "                animals = [a.strip() for a in animals_raw.split(\",\") if a.strip()]\n",
        "        elif isinstance(animals_raw, list):\n",
        "            animals = animals_raw\n",
        "        else:\n",
        "            animals = []\n",
        "        # actions\n",
        "        actions = []\n",
        "        if isinstance(actions_raw, str) and actions_raw.strip():\n",
        "            try:\n",
        "                parsed = ast.literal_eval(actions_raw)\n",
        "                if isinstance(parsed, list):\n",
        "                    for it in parsed:\n",
        "                        if isinstance(it, (list, tuple)) and len(it) >= 2: actions.append(str(it[1]))\n",
        "                        elif isinstance(it, str): actions.append(it)\n",
        "            except Exception:\n",
        "                actions = [a.strip() for a in actions_raw.split(\",\") if a.strip()]\n",
        "        meta[vid] = {\"animals\": animals, \"actions\": actions}\n",
        "    return meta\n",
        "\n",
        "metadata = parse_metadata(meta_df)\n",
        "\n",
        "# ================================\n",
        "# 8) Robust semantic scoring (compact)\n",
        "# ================================\n",
        "ANIMAL_BACKOFF = {\n",
        "    \"nightingale\":[\"nightingale\",\"bird\",\"songbird\"],\n",
        "    \"wren\":[\"wren\",\"bird\"], \"egret\":[\"egret\",\"heron\",\"bird\"], \"heron\":[\"heron\",\"bird\"],\n",
        "    \"stilt\":[\"stilt\",\"wader\",\"shorebird\",\"bird\"], \"crane\":[\"crane\",\"bird\"],\n",
        "    \"duck\":[\"duck\",\"waterfowl\",\"bird\"], \"tit\":[\"tit\",\"bird\"],\n",
        "    \"snake\":[\"snake\",\"serpent\",\"reptile\"], \"mongoose\":[\"mongoose\",\"mammal\",\"animal\"],\n",
        "    \"bird\":[\"bird\"], \"animal\":[\"animal\"],\n",
        "}\n",
        "\n",
        "def normalize_lemmas(text):\n",
        "    words = re.findall(r\"[a-zA-Z]+\", (text or \"\").lower())\n",
        "    return {lemmatizer.lemmatize(w, pos='n') for w in words} | {lemmatizer.lemmatize(w, pos='v') for w in words}\n",
        "\n",
        "def partial_match(caption, phrase, thr=0.6):\n",
        "    cap = normalize_lemmas(caption)\n",
        "    toks = [t for t in re.findall(r\"[a-zA-Z]+\", (phrase or \"\").lower()) if t]\n",
        "    if not toks: return False\n",
        "    kw = {lemmatizer.lemmatize(t, pos='n') for t in toks} | {lemmatizer.lemmatize(t, pos='v') for t in toks}\n",
        "    return (sum(1 for t in kw if t in cap) / len(kw)) >= thr\n",
        "\n",
        "def animal_ok(caption, key):\n",
        "    cap = (caption or \"\").lower()\n",
        "    cand = ANIMAL_BACKOFF.get(key, [key])\n",
        "    return any(c in cap for c in cand) or partial_match(caption, key, thr=0.5)\n",
        "\n",
        "def infer_actions(text):\n",
        "    t = (text or \"\").lower()\n",
        "    inferred=set()\n",
        "    def has(xs): return any(x in t for x in xs)\n",
        "    if has([\"singing\",\"chirping\",\"trilling\",\"calling\",\"vocalizing\",\"song\"]): inferred.update([\"singing\",\"chirping\",\"calling\"])\n",
        "    if has([\"beak open\",\"open beak\",\"mouth open\",\"open mouth\"]): inferred.update([\"singing\",\"calling\",\"chirping\"])\n",
        "    if has([\"keeping still\",\"standing still\",\"motionless\",\"perched\",\"sitting\",\"resting\",\"standing\"]): inferred.update([\"keeping still\",\"standing still\",\"looking\"])\n",
        "    if \"walking\" in t: inferred.add(\"walking\")\n",
        "    if \"running\" in t: inferred.add(\"running\")\n",
        "    if has([\"flying\",\"in flight\",\"soars\",\"soaring\",\"taking off\",\"takes off\"]): inferred.add(\"flying\")\n",
        "    if has([\"landing\",\"lands\",\"touches down\"]): inferred.add(\"landing\")\n",
        "    if has([\"flapping\",\"flaps\"]): inferred.add(\"flapping\")\n",
        "    if has([\"eating\",\"feeding\",\"chewing\",\"grazing\",\"pecking\"]):\n",
        "        inferred.add(\"eating\")\n",
        "        if has([\"eating a\",\"eating the\",\"eating prey\",\"eating a snake\",\"eats a\"]): inferred.update([\"biting\",\"attacking\"])\n",
        "    if has([\"biting\",\"bites\",\"bite\"]): inferred.update([\"biting\",\"attacking\"])\n",
        "    if has([\"dead \",\" carcass\",\"lifeless\"]): inferred.add(\"dying\")\n",
        "    if has([\"shaking head\",\"head shake\"]): inferred.add(\"shaking head\")\n",
        "    if has([\"looking\",\"watching\",\"peering\",\"gazing\",\"staring\"]): inferred.add(\"looking\")\n",
        "    if \"moving\" in t: inferred.add(\"moving\")\n",
        "    return inferred\n",
        "\n",
        "def score_semantics(caption, animals_norm, actions_norm):\n",
        "    keywords = [*(animals_norm or []), *(actions_norm or [])]\n",
        "    keywords = [k for k in keywords if isinstance(k, str) and k.strip()]\n",
        "    if not keywords:\n",
        "        return \"NA\", [], [], \"NA\", [], []\n",
        "    cap_l = (caption or \"\").lower()\n",
        "\n",
        "    # strict (substring)\n",
        "    s_match, s_miss = [], []\n",
        "    for kw in keywords:\n",
        "        (s_match if kw.lower() in cap_l else s_miss).append(kw)\n",
        "    s_pct = f\"{(len(s_match)/len(keywords))*100:.1f}%\"\n",
        "\n",
        "    # robust (partial + inference + hypernym)\n",
        "    inf = infer_actions(caption)\n",
        "    r_match, r_miss = [], []\n",
        "    for kw in keywords:\n",
        "        k = kw.lower()\n",
        "        if k in (animals_norm or []):\n",
        "            ok = animal_ok(caption, k)\n",
        "        else:\n",
        "            ok = (k in inf) or partial_match(caption, k, thr=0.6)\n",
        "        (r_match if ok else r_miss).append(kw)\n",
        "    r_pct = f\"{(len(r_match)/len(keywords))*100:.1f}%\"\n",
        "    return s_pct, s_match, s_miss, r_pct, r_match, r_miss\n",
        "\n",
        "# ================================\n",
        "# 9) Random sampling of 1000 videos (reproducible)\n",
        "# ================================\n",
        "video_dir = \"/content/videos/video/\"\n",
        "all_videos = [f for f in os.listdir(video_dir) if f.endswith(\".mp4\")]\n",
        "\n",
        "SAMPLE_SIZE = 1000\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "if len(all_videos) <= SAMPLE_SIZE:\n",
        "    video_files = all_videos[:]  # take all if fewer than requested\n",
        "else:\n",
        "    video_files = random.sample(all_videos, SAMPLE_SIZE)\n",
        "\n",
        "pd.DataFrame({\"video_file\": video_files}).to_csv(\"/content/sampled_videos.csv\", index=False)\n",
        "print(f\"Sampling {len(video_files)} videos (seed={RANDOM_SEED}). Saved list to /content/sampled_videos.csv.\")\n",
        "\n",
        "# ================================\n",
        "# 10) Main (batched, capped, progress saved at the end)\n",
        "# ================================\n",
        "SUMMARIZER = \"flan\"           # \"flan\" (recommended) or \"bart\"\n",
        "USE_KEYWORD_STEERING = True\n",
        "MAX_FRAMES_PER_VIDEO = 96\n",
        "BATCH_SIZE = 4\n",
        "OUT_CSV = \"/content/summary_results_r1000.csv\"\n",
        "\n",
        "rows = []\n",
        "t_start = time.time()\n",
        "\n",
        "for idx, vf in enumerate(video_files, 1):\n",
        "    vid = os.path.splitext(vf)[0]\n",
        "    vpath = os.path.join(video_dir, vf)\n",
        "    fdir = f\"/content/frames/{vid}\"\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n[{idx}/{len(video_files)}] Processing {vid}\")\n",
        "        n = extract_frames_4fps(vpath, fdir, max_frames=MAX_FRAMES_PER_VIDEO)\n",
        "        if n == 0:\n",
        "            print(\"  ! No frames extracted; skipping.\")\n",
        "            continue\n",
        "\n",
        "        frame_names = sorted([fn for fn in os.listdir(fdir) if fn.endswith(\".jpg\")])\n",
        "        frame_paths = [os.path.join(fdir, fn) for fn in frame_names]\n",
        "\n",
        "        # Batched BLIP2\n",
        "        frame_captions = batch_generate_captions(frame_paths, batch_size=BATCH_SIZE, max_new_tokens=35)\n",
        "\n",
        "        # Metadata -> normalized keywords\n",
        "        m = metadata.get(vid, {\"animals\": [], \"actions\": []})\n",
        "        animals_raw, actions_raw = m.get(\"animals\", []), m.get(\"actions\", [])\n",
        "        animals_n, actions_n = normalize_meta_keywords(animals_raw, actions_raw)\n",
        "        meta_keywords_norm = [*animals_n, *actions_n]\n",
        "\n",
        "        encourage = pick_keywords_to_encourage(frame_captions, meta_keywords_norm) if USE_KEYWORD_STEERING else None\n",
        "\n",
        "        # Summarize\n",
        "        if SUMMARIZER == \"flan\":\n",
        "            final_caption = summarize_flan(frame_captions, encourage=encourage, max_new_tokens=60)\n",
        "        else:\n",
        "            final_caption = summarize_bart(frame_captions, encourage=encourage, max_length=60)\n",
        "\n",
        "        # Scores\n",
        "        s_pct, s_match, s_miss, r_pct, r_match, r_miss = score_semantics(final_caption, animals_n, actions_n)\n",
        "\n",
        "        rows.append({\n",
        "            \"video_id\": vid,\n",
        "            \"final_caption\": final_caption,\n",
        "            \"frame_captions_curated\": curate_captions(frame_captions, max_caps=80),\n",
        "            \"keywords_normalized\": meta_keywords_norm,\n",
        "            \"strict_matched\": s_match, \"strict_missing\": s_miss, \"strict_semantic_percent\": s_pct,\n",
        "            \"robust_matched\": r_match, \"robust_missing\": r_miss, \"robust_semantic_percent\": r_pct\n",
        "        })\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n!! Interrupted by user.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"  !! Error on {vid}: {e}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "# Save results\n",
        "pd.DataFrame(rows).to_csv(OUT_CSV, index=False)\n",
        "print(f\"\\nDone. Saved {len(rows)} rows to {OUT_CSV}. Total time: {time.time()-t_start:.1f}s\")\n"
      ],
      "metadata": {
        "id": "T4troS3OfxHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to your results CSV\n",
        "csv_path = \"/content/summary_results_r1000.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "def to_float(series):\n",
        "    \"\"\"Convert '73.5%' -> 73.5, handle NA/empty.\"\"\"\n",
        "    return (\n",
        "        series.astype(str)\n",
        "        .str.replace(\"%\", \"\", regex=False)\n",
        "        .str.strip()\n",
        "        .replace({\"NA\": None})\n",
        "        .astype(float)\n",
        "    )\n",
        "\n",
        "stats = {}\n",
        "\n",
        "for col in [\"strict_semantic_percent\", \"robust_semantic_percent\"]:\n",
        "    if col in df.columns:\n",
        "        vals = to_float(df[col]).dropna()\n",
        "        if not vals.empty:\n",
        "            stats[col] = {\n",
        "                \"count\": int(vals.count()),\n",
        "                \"mean\": float(vals.mean()),\n",
        "                \"median\": float(vals.median()),\n",
        "                \"std\": float(vals.std(ddof=1)),\n",
        "                \"min\": float(vals.min()),\n",
        "                \"max\": float(vals.max()),\n",
        "                \"zero_count\": int((vals == 0).sum())\n",
        "            }\n",
        "\n",
        "# Print results\n",
        "for col, m in stats.items():\n",
        "    print(f\"\\n{col}:\")\n",
        "    for k, v in m.items():\n",
        "        print(f\"  {k}: {v:.2f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "T8pUupVC61MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_DJw6WCuEagB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mV3OmkRJEacb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pyK_KngwEaZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizations"
      ],
      "metadata": {
        "id": "vaBrMJpFViHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Colab Visualization Script for Semantic Scoring (Sequential 1000 vs Random 1000) ---\n",
        "# Input files (change these names if yours differ)\n",
        "SEQ_CSV = \"summary_results (1).csv\"     # first 1000, sequential\n",
        "RAND_CSV = \"summary_results_r1000.csv\"  # random 1000\n",
        "\n",
        "# 1) Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2) Load CSVs\n",
        "def load_results(path):\n",
        "    df = pd.read_csv(path)\n",
        "    # Coerce percent strings like \"50.0%\" -> float\n",
        "    def to_float_percent(colname):\n",
        "        if colname in df.columns:\n",
        "            return (\n",
        "                df[colname]\n",
        "                .astype(str)\n",
        "                .str.replace(\"%\", \"\", regex=False)\n",
        "                .str.strip()\n",
        "                .replace({\"nan\": np.nan, \"None\": np.nan})\n",
        "                .astype(float)\n",
        "            )\n",
        "        # If missing, create NaNs so code still runs\n",
        "        return pd.Series([np.nan] * len(df), index=df.index)\n",
        "\n",
        "    # Standardize columns (observed in your CSVs)\n",
        "    df[\"strict_semantic_percent_num\"] = to_float_percent(\"strict_semantic_percent\")\n",
        "    df[\"robust_semantic_percent_num\"] = to_float_percent(\"robust_semantic_percent\")\n",
        "    return df\n",
        "\n",
        "df_seq = load_results(SEQ_CSV)\n",
        "df_rand = load_results(RAND_CSV)\n",
        "\n",
        "# Quick sanity checks\n",
        "print(\"Sequential CSV rows:\", len(df_seq))\n",
        "print(\"Random CSV rows:\", len(df_rand))\n",
        "print(\"Seq strict/robust head:\\n\", df_seq[[\"strict_semantic_percent_num\",\"robust_semantic_percent_num\"]].head())\n",
        "print(\"Rand strict/robust head:\\n\", df_rand[[\"strict_semantic_percent_num\",\"robust_semantic_percent_num\"]].head())\n",
        "\n",
        "# 3) Create output folder\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "\n",
        "# 4) Helper: summary stats table\n",
        "def summary_stats(name, series):\n",
        "    series = series.dropna()\n",
        "    return pd.Series(\n",
        "        {\n",
        "            \"count\": len(series),\n",
        "            \"mean\": series.mean(),\n",
        "            \"median\": series.median(),\n",
        "            \"std\": series.std(ddof=1),\n",
        "            \"min\": series.min(),\n",
        "            \"max\": series.max(),\n",
        "            \"zeros\": (series == 0).sum(),\n",
        "        },\n",
        "        name=name,\n",
        "    )\n",
        "\n",
        "stats = pd.concat(\n",
        "    [\n",
        "        summary_stats(\"seq_strict\", df_seq[\"strict_semantic_percent_num\"]),\n",
        "        summary_stats(\"seq_robust\", df_seq[\"robust_semantic_percent_num\"]),\n",
        "        summary_stats(\"rand_strict\", df_rand[\"strict_semantic_percent_num\"]),\n",
        "        summary_stats(\"rand_robust\", df_rand[\"robust_semantic_percent_num\"]),\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "print(\"\\n=== Summary Stats (percent) ===\\n\")\n",
        "print(stats.round(2))\n",
        "\n",
        "# 5) Histogram(s): strict vs robust for each dataset\n",
        "def hist_two(series_a, series_b, labels, title, fname):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    bins = np.linspace(0, 100, 21)  # 0..100 in 5% steps\n",
        "    plt.hist(series_a.dropna(), bins=bins, alpha=0.6, label=labels[0])\n",
        "    plt.hist(series_b.dropna(), bins=bins, alpha=0.6, label=labels[1])\n",
        "    plt.xlabel(\"Semantic Percent (%)\")\n",
        "    plt.ylabel(\"Count of Videos\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(\"figures\", fname), dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "# Sequential 1000\n",
        "hist_two(\n",
        "    df_seq[\"strict_semantic_percent_num\"],\n",
        "    df_seq[\"robust_semantic_percent_num\"],\n",
        "    labels=[\"Strict (Sequential 1000)\", \"Robust (Sequential 1000)\"],\n",
        "    title=\"Sequential 1000: Strict vs Robust Semantic Score Distribution\",\n",
        "    fname=\"hist_seq_strict_vs_robust.png\",\n",
        ")\n",
        "\n",
        "# Random 1000\n",
        "hist_two(\n",
        "    df_rand[\"strict_semantic_percent_num\"],\n",
        "    df_rand[\"robust_semantic_percent_num\"],\n",
        "    labels=[\"Strict (Random 1000)\", \"Robust (Random 1000)\"],\n",
        "    title=\"Random 1000: Strict vs Robust Semantic Score Distribution\",\n",
        "    fname=\"hist_rand_strict_vs_robust.png\",\n",
        ")\n",
        "\n",
        "# 6) Boxplots: compare sequential vs random for each metric\n",
        "def boxplot_compare(seq_series, rand_series, title, ylabel, fname):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    data = [seq_series.dropna(), rand_series.dropna()]\n",
        "    plt.boxplot(data, labels=[\"Sequential 1000\", \"Random 1000\"], showmeans=True)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(\"figures\", fname), dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "boxplot_compare(\n",
        "    df_seq[\"strict_semantic_percent_num\"],\n",
        "    df_rand[\"strict_semantic_percent_num\"],\n",
        "    \"Strict Semantic Score: Sequential vs Random\",\n",
        "    \"Strict Semantic Percent (%)\",\n",
        "    \"boxplot_strict_seq_vs_rand.png\",\n",
        ")\n",
        "\n",
        "boxplot_compare(\n",
        "    df_seq[\"robust_semantic_percent_num\"],\n",
        "    df_rand[\"robust_semantic_percent_num\"],\n",
        "    \"Robust Semantic Score: Sequential vs Random\",\n",
        "    \"Robust Semantic Percent (%)\",\n",
        "    \"boxplot_robust_seq_vs_rand.png\",\n",
        ")\n",
        "\n",
        "# 7) Scatter: strict vs robust (per dataset)\n",
        "def scatter_strict_vs_robust(df, title, fname):\n",
        "    x = df[\"strict_semantic_percent_num\"]\n",
        "    y = df[\"robust_semantic_percent_num\"]\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.scatter(x, y, s=12, alpha=0.5)\n",
        "    # y = x reference line\n",
        "    lims = [0, 100]\n",
        "    plt.plot(lims, lims, 'k--', linewidth=1)\n",
        "    plt.xlim(lims)\n",
        "    plt.ylim(lims)\n",
        "    plt.xlabel(\"Strict Semantic Percent (%)\")\n",
        "    plt.ylabel(\"Robust Semantic Percent (%)\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(\"figures\", fname), dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "scatter_strict_vs_robust(df_seq, \"Sequential 1000: Strict vs Robust (per video)\", \"scatter_seq_strict_vs_robust.png\")\n",
        "scatter_strict_vs_robust(df_rand, \"Random 1000: Strict vs Robust (per video)\", \"scatter_rand_strict_vs_robust.png\")\n",
        "\n",
        "# 8) Bar chart: zero-score counts\n",
        "def zero_count(series):\n",
        "    return int((series.dropna() == 0).sum())\n",
        "\n",
        "z_seq_strict = zero_count(df_seq[\"strict_semantic_percent_num\"])\n",
        "z_seq_robust = zero_count(df_seq[\"robust_semantic_percent_num\"])\n",
        "z_rand_strict = zero_count(df_rand[\"strict_semantic_percent_num\"])\n",
        "z_rand_robust = zero_count(df_rand[\"robust_semantic_percent_num\"])\n",
        "\n",
        "labels = [\"Seq Strict\", \"Seq Robust\", \"Rand Strict\", \"Rand Robust\"]\n",
        "zeros = [z_seq_strict, z_seq_robust, z_rand_strict, z_rand_robust]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "bars = plt.bar(labels, zeros)\n",
        "plt.ylabel(\"Count of Videos with 0% Score\")\n",
        "plt.title(\"Zero-Score Counts: Strict vs Robust, Sequential vs Random\")\n",
        "plt.grid(True, axis=\"y\", alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(\"figures\", \"bar_zeros_strict_vs_robust.png\"), dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nZero-score counts:\")\n",
        "for lab, z in zip(labels, zeros):\n",
        "    print(f\"  {lab}: {z}\")\n",
        "\n",
        "# 9) Save the summary stats to CSV for easy inclusion in the dissertation\n",
        "stats.round(2).to_csv(\"figures/summary_stats_semantic_scores.csv\")\n",
        "print(\"\\nSaved figures in ./figures and stats CSV at figures/summary_stats_semantic_scores.csv\")\n"
      ],
      "metadata": {
        "id": "fhv1QZurVj2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load your Exp-5 CSV\n",
        "df = pd.read_csv(\"blip2_bart_results.csv\")\n",
        "\n",
        "# Convert semantic correctness to float\n",
        "df[\"semantic_correctness_percent_num\"] = (\n",
        "    df[\"semantic_correctness_percent\"]\n",
        "    .astype(str)\n",
        "    .str.replace(\"%\", \"\", regex=False)\n",
        "    .astype(float)\n",
        ")\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(8,6))\n",
        "bins = np.linspace(0, 100, 21)  # 0 to 100 in 5% bins\n",
        "plt.hist(df[\"semantic_correctness_percent_num\"], bins=bins, color=\"skyblue\", edgecolor=\"black\", alpha=0.7)\n",
        "plt.xlabel(\"Semantic Correctness Score (%)\")\n",
        "plt.ylabel(\"Number of Videos\")\n",
        "plt.title(\"Distribution of Semantic Scores (Exp-5: BLIP2+BART @ 4FPS+Prompt, N=1000)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BXUzqMSRb-QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"blip2_bart_results.csv\")\n",
        "\n",
        "# Convert semantic correctness to float\n",
        "df[\"semantic_correctness_percent_num\"] = (\n",
        "    df[\"semantic_correctness_percent\"]\n",
        "    .astype(str)\n",
        "    .str.replace(\"%\", \"\", regex=False)\n",
        "    .astype(float)\n",
        ")\n",
        "\n",
        "# Define bins (0-100 in 10% intervals)\n",
        "bins = np.arange(0, 110, 10)  # 0,10,20,...100\n",
        "labels = [f\"{bins[i]}–{bins[i+1]}%\" for i in range(len(bins)-1)]\n",
        "\n",
        "# Cut into bins\n",
        "df[\"score_range\"] = pd.cut(df[\"semantic_correctness_percent_num\"], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "# Count how many videos fall into each bin\n",
        "distribution = df[\"score_range\"].value_counts().sort_index()\n",
        "\n",
        "# Convert to DataFrame for clean table\n",
        "dist_table = pd.DataFrame({\n",
        "    \"Score Range\": distribution.index,\n",
        "    \"Number of Videos\": distribution.values\n",
        "})\n",
        "\n",
        "print(dist_table.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "o1Pnbvu6dH1n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}